{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOKUkE+PGTuSlIHeCZB42N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yougoal/leejunghoon/blob/master/dqn2015r3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnS-gBNB2XBS",
        "colab_type": "code",
        "outputId": "2fe79659-2149-4a3f-a8d9-94780df4ef3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "#참조: https://hunkim.github.io/ml/\n",
        "#setup\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random \n",
        "from collections import deque\n",
        "import gym\n",
        "\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "dis = 1\n",
        "#REPLAY_MEMORY =5\n",
        "class DQN:\n",
        "    def __init__(self, session, input_size, output_size, name=\"main\"):\n",
        "        self.session = session\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.net_name = name\n",
        "        self.build_network()\n",
        "    def build_network(self, h_size=10, l_rate=1e-1):\n",
        "        with tf.variable_scope(self.net_name):\n",
        "            self.X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
        "            print(\"훈련에 사용되는  self._X: St\")\n",
        "            print(self.X)\n",
        "            self.W1 = tf.get_variable('W1', shape = [input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
        "            print(\"훈련함여한 W1\")\n",
        "            print(self.W1)\n",
        "            self.Qpred = tf.matmul(self.X, self.W1)\n",
        "        self.Y = tf.placeholder(shape = [None, self.output_size], dtype = tf.float32)\n",
        "        self.loss = tf.reduce_mean(tf.square(self.Y - self.Qpred))\n",
        "        self.train = tf.train.AdamOptimizer(learning_rate=l_rate).minimize(self.loss)\n",
        "    def predict(self, state):\n",
        "        x = np.reshape(state, [1, self.input_size])\n",
        "        return self.session.run(self.Qpred, feed_dict={self.X: x})\n",
        "    def update(self, x_stack, y_stack):\n",
        "        return self.session.run([self.loss, self.train], feed_dict={self.X: x_stack, self.Y: y_stack})\n",
        "def replay_train(mainDQN, targetDQN, train_batch):\n",
        "    x_stack = np.empty(0).reshape(0, 4)  # STACK 정의\n",
        "    y_stack = np.empty(0).reshape(0, 2)  # STACK 정의\n",
        "    for state, action, reward, next_state, done in train_batch:\n",
        "        Q = mainDQN.predict(state)       \n",
        "        print(\"이때 사용된 mainDQN의 W 및 st\")\n",
        "        print(sess.run(mainDQN._W1))\n",
        "        print(\"이때 사용된  St\")\n",
        "        print(state)\n",
        "        print(\"mainDQN의 예측값 stW 출력\")\n",
        "        print(Q)\n",
        "        if done:\n",
        "            Q[0, action] = reward\n",
        "        else:\n",
        "            Q[0, action] = reward + dis * np.max(targetDQN.predict(next_state))\n",
        "            print(\"이때 사용된 Target DQN W\")\n",
        "            print(sess.run(targetDQN._W1))\n",
        "            print(\"이때 사용된  St+1\")\n",
        "            print(next_state)\n",
        "            print(\"Target DQN의 예측값 stW 출력\")\n",
        "            print(Q)\n",
        "        y_stack = np.vstack([y_stack, Q])\n",
        "        x_stack = np.vstack([x_stack, state])\n",
        "        print(\"Stack에 누적되는 state St\")\n",
        "        print(x_stack)\n",
        "        print(\"Stack에 누적되는 target Y\")\n",
        "        print(y_stack)\n",
        "    return mainDQN.update(x_stack, y_stack)\n",
        "    \n",
        "def bot_play(mainDQN):\n",
        "    s = env.reset()\n",
        "    reward_sum = 0\n",
        "    while True:\n",
        "        #env.render()\n",
        "        a = np.argmax(mainDQN.predict(s))\n",
        "        s, reward, done, _ = env.step(a)\n",
        "        reward_sum += reward\n",
        "        if done:\n",
        "            print(\"Total score: {}\".format(reward_sum))\n",
        "            break\n",
        "def get_copy_var_ops(*, dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
        "    op_holder = []\n",
        "    src_vals = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = src_scope_name)\n",
        "    dest_vals = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = dest_scope_name)\n",
        "    for src_vals, dest_vals in zip(src_vals, dest_vals):\n",
        "        print(\"메인22W\")\n",
        "        print(src_vals, dest_vals)\n",
        "        op_holder.append(dest_vals.assign(src_vals.value()))\n",
        "    return op_holder\n",
        "\n",
        "max_episodes =9\n",
        "#replay_buffer = deque()\n",
        "replay_buffer = []\n",
        "with tf.Session() as sess:\n",
        "    mainDQN = DQN(sess, input_size, output_size, name=\"main\")\n",
        "    targetDQN = DQN(sess, input_size, output_size, name=\"target\")\n",
        "    tf.global_variables_initializer().run()\n",
        "    copy_ops = get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\")\n",
        "    sess.run(copy_ops)\n",
        "    print(\"초기네트워크 mian\")\n",
        "    print(sess.run(mainDQN._W1))\n",
        "    print(\"초기네트워크 target\")\n",
        "    print(sess.run(targetDQN._W1))\n",
        "    \n",
        "    for episode in range(max_episodes):\n",
        "        e = 1. / ((episode / 10) + 1)\n",
        "        done = False\n",
        "        step_count = 0\n",
        "\n",
        "        state = env.reset()\n",
        "        print(\"episode: {}\".format(episode))\n",
        "        print(\"Step마다 mianW\")\n",
        "        print(sess.run(mainDQN._W1))\n",
        "        print(\"Sep 마다 targetw\")\n",
        "        print(sess.run(targetDQN._W1))\n",
        "        while not done:\n",
        "            if np.random.rand(1) < e :\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(mainDQN.predict(state))\n",
        "            print(\" 사용된 mian_W\")\n",
        "            print(sess.run(mainDQN._W1))\n",
        "            print(\"Episode\",episode)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            print(\"s_t: {} action: {} Reward{} \".format(state,action,reward))\n",
        "            print(\"st+1: {}  done{}\".format(next_state,done))\n",
        "            #if done:\n",
        "            #    reward = -100\n",
        "            #REPLAY_MEMORY.append([state, action, reward, next_state, done])\n",
        "            replay_buffer.append([state, action, reward, next_state, done])\n",
        "            print(\"메모리\",replay_buffer[0:1])\n",
        "            print(\"메모리\",replay_buffer[1:2])\n",
        "            print(\"메모리\",replay_buffer[2:3])\n",
        "            print(\"메모리\",replay_buffer[3:4])\n",
        "            print(\"메모리\",replay_buffer[4:5])\n",
        "            print(\"메모리\",replay_buffer[5:6])\n",
        "            if len(replay_buffer) > 5:\n",
        "               del replay_buffer[0]\n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "            if step_count > 10000:\n",
        "                break\n",
        "        print(\"episode: {}  step: {}\".format(episode, step_count))\n",
        "\n",
        "        if step_count > 10000:\n",
        "            pass\n",
        "        if episode % 4 == 3:\n",
        "            for _ in range(3):\n",
        "                minibatch = random.sample(replay_buffer, 2)\n",
        "                loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n",
        "                print(\"episode: {}\".format(episode))\n",
        "                print(\"중간훈련후 mianDQN W\")\n",
        "                print(sess.run(mainDQN._W1))\n",
        "                print(\"중간훈련후 TargetDQN W\")\n",
        "                print(sess.run(targetDQN._W1))\n",
        "            print(\"Loss: \", loss)\n",
        "            sess.run(copy_ops)\n",
        "            print(\"훈련된 Wm과 새로시작할때Wt로 이동시켜 동일하게 함.Wt<=Wm\",sess.run(copy_ops))\n",
        "    bot_play(mainDQN)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "훈련에 사용되는  self._X: St\n",
            "Tensor(\"main/input_x:0\", shape=(?, 4), dtype=float32)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "훈련함여한 W1\n",
            "<tf.Variable 'main/W1:0' shape=(4, 2) dtype=float32_ref>\n",
            "훈련에 사용되는  self._X: St\n",
            "Tensor(\"target/input_x:0\", shape=(?, 4), dtype=float32)\n",
            "훈련함여한 W1\n",
            "<tf.Variable 'target/W1:0' shape=(4, 2) dtype=float32_ref>\n",
            "메인22W\n",
            "<tf.Variable 'main/W1:0' shape=(4, 2) dtype=float32_ref> <tf.Variable 'target/W1:0' shape=(4, 2) dtype=float32_ref>\n",
            "초기네트워크 mian\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-be3c53cdad6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"초기네트워크 mian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_W1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"초기네트워크 target\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_W1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DQN' object has no attribute '_W1'"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUnVp0GIIrTIggR2hVF7lo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yougoal/leejunghoon/blob/master/DQN2015_other.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk7HgTUr_rSj",
        "colab_type": "code",
        "outputId": "ee1d8307-ea4e-4a1e-ff77-c8a51c720450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2296
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from gym import wrappers\n",
        "import numpy as np\n",
        "import random as ran\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# 꺼내서 사용할 리플레이 갯수\n",
        "REPLAY = 3\n",
        "# 리플레이를 저장할 리스트\n",
        "REPLAY_MEMORY = []\n",
        "# 미니배치\n",
        "MINIBATCH = 5\n",
        "\n",
        "INPUT = env.observation_space.shape[0]\n",
        "OUTPUT = env.action_space.n\n",
        "\n",
        "# 하이퍼파라미터\n",
        "LEARNING_LATE = 0.001\n",
        "DISCOUNT = 0.99\n",
        "model_path = \"save/model.ckpt\"\n",
        "\n",
        "\n",
        "# 두개의 네트워크 구성\n",
        "\n",
        "x=tf.placeholder(dtype=tf.float32, shape=(None, 4))\n",
        "Y=tf.placeholder(dtype=tf.float32, shape=(None, 2))\n",
        "\n",
        "\n",
        "# Main 네트워크\n",
        "W1 = tf.get_variable('W1',shape=[4,2],initializer=tf.contrib.layers.xavier_initializer())\n",
        "Q_pre = tf.matmul(x,W1)\n",
        "\n",
        "# Target 네트워크\n",
        "W1_r = tf.get_variable('W1_r',shape=[4,2],initializer=tf.contrib.layers.xavier_initializer())\n",
        "Q_pre_r = tf.matmul(x,W1_r)\n",
        "\n",
        "# 총 Reward를 저장해놓을 리스트\n",
        "rlist=[0]\n",
        "recent_rlist=[0]\n",
        "\n",
        "episode = 0\n",
        "\n",
        "# Loss function 정의\n",
        "cost = tf.reduce_sum(tf.square(Y-Q_pre))\n",
        "optimizer = tf.train.AdamOptimizer(LEARNING_LATE, epsilon=0.01)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# 세션 정의\n",
        "with tf.Session() as sess:\n",
        "    # 변수 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Target 네트워크에 main 네트워크 값을 카피해줌\n",
        "    sess.run(W1_r.assign(W1))\n",
        "    print(\"초기시작시 Wm과 Wt확인\")\n",
        "    print(sess.run(W1_r.assign(W1)))\n",
        "    print(sess.run(W1))\n",
        "\n",
        "    # 에피소드 시작\n",
        "    while np.mean(recent_rlist) <10 :\n",
        "        episode += 1\n",
        "\n",
        "        # state 초기화\n",
        "        s = env.reset()\n",
        "        if len(recent_rlist) > 5:\n",
        "            del recent_rlist[0]\n",
        "        print(\"recent_rlist\")\n",
        "        print(recent_rlist)\n",
        "        # e-greedy\n",
        "        e = 1. / ((episode/25)+1)\n",
        "\n",
        "        rall = 0\n",
        "        d = False\n",
        "        count = 0\n",
        "\n",
        "        # 에피소드가 끝나기 전까지 반복\n",
        "        while not d  :\n",
        "\n",
        "            #env.render()\n",
        "            count += 1\n",
        "\n",
        "            # state 값의 전처리\n",
        "            s_t = np.reshape(s,[1,INPUT])\n",
        "            # 현재 상태의 Q값을 에측\n",
        "            Q = sess.run(Q_pre, feed_dict={x:s_t})\n",
        "            # e-greedy 정책으로 랜덤하게 action 결정\n",
        "            if e > np.random.rand(1):\n",
        "                a = env.action_space.sample()\n",
        "            else:\n",
        "                a = np.argmax(Q)\n",
        "\n",
        "            # 결정된 action으로 Environment에 입력\n",
        "            s1, r, d, _ = env.step(a)\n",
        "\n",
        "            # Environment에서 반환한 Next_state, action, reward, done 값들을\n",
        "            # Replay_memory에 저장\n",
        "            REPLAY_MEMORY.append([s_t,a,r,s1,d,count])\n",
        "\n",
        "            # 저장된 값들이 50000개 이상 넘어가면 맨 앞 Replay부터 삭제\n",
        "            if len(REPLAY_MEMORY) > 5:\n",
        "                del REPLAY_MEMORY[0]\n",
        "            print('메모리 버퍼에 저장된 값 출력')\n",
        "           # print(REPLAY_MEMORY[0:1])\n",
        "            #print(REPLAY_MEMORY[1:2])\n",
        "            #print(REPLAY_MEMORY[2:3])\n",
        "            #print(REPLAY_MEMORY[3:4])\n",
        "            #print(REPLAY_MEMORY[4:5])\n",
        "            #print(REPLAY_MEMORY[5:6])\n",
        "            # 총 reward 합\n",
        "            rall += r\n",
        "            # state를 Next_state로 바꿈\n",
        "            s = s1\n",
        "\n",
        "\n",
        "        # 일정간격의 episode마다 학습을 수행한다.\n",
        "        if episode % 5 == 2 :\n",
        "\n",
        "            # 50번의 미니배치로 학습\n",
        "                # 저장된 리플레이 중에 학습에 사용할 랜덤한 리플레이 샘플들을 가져옴\n",
        "            for sample in ran.sample(REPLAY_MEMORY, 2):\n",
        "                s_t_r, a_r, r_r, s1_r, d_r ,count_r= sample\n",
        "                print(\"메모리버퍼에서 꺼내온 샘플 Target Str At Rt St+1 done count_R\")\n",
        "                print(sample)\n",
        "                # 꺼내온 리플레이의 state의 Q값을 예측\n",
        "                Q = sess.run(Q_pre, feed_dict={x: s_t_r})\n",
        "                print(\"메모리버퍼에서 꺼내온 St로 Q(St,a)를 예측 Qpre=st*Wm\")\n",
        "                print(sess.run(W1))\n",
        "                if d_r:\n",
        "                    # 꺼내온 리플레이의 상태가 끝난 상황이라면 Negative Reward를 부여\n",
        "                        Q[0, a_r] = -100\n",
        "                else:\n",
        "                    # 끝나지 않았다면 Q값을 업데이트\n",
        "                    # 꺼내온 st+1로 표적의 값을 만들어낸다\n",
        "                    s1_t_r= np.reshape(s1_r,[1,4])\n",
        "                    Q1 = sess.run(Q_pre_r, feed_dict={x: s1_t_r})\n",
        "                    print(\"y(st.a)에 사용된 Wt\")\n",
        "                    print(sess.run(W1))\n",
        "                    Q[0, a_r]=np.max(Q1)\n",
        "                    # 표적값\n",
        "                    print(\"표적 Q(st+1,at)==>y(st,a)=st+1*Wt, Q1 및 Q\")\n",
        "                    print(Q1,Q)\n",
        "                    \n",
        "                # 업데이트 된 Q값으로 main네트워크를 학습\n",
        "                _, loss = sess.run([train, cost], feed_dict={x: s_t_r, Y: Q })\n",
        "                print(\"훈련에참여한 값 St{}\".format(s_t_r))\n",
        "                print(\"훈련에참여한 값 Target{}\".format(Q))\n",
        "                print(\" REPLY 훈련W값\")\n",
        "                print(sess.run([W1]))\n",
        "                print(\"최종 loss{}\".format(loss))\n",
        "                \n",
        "            # 10번 마다 target 네트워크에 main 네트워크 값을 copy\n",
        "            sess.run(W1_r.assign(W1))\n",
        "            print(\"Wm==>Wt로 복사\")\n",
        "            print(sess.run(W1_r.assign(W1)))\n",
        "        # 총 reward의 합을 list에 저장\n",
        "        recent_rlist.append(rall)\n",
        "        rlist.append(rall)\n",
        "        print(\"Episode:{} steps:{} reward:{} average reward:{} recent reward:{}\".format(episode, count, rall,\n",
        "                                                                                        np.mean(rlist),\n",
        "                                                                                        np.mean(recent_rlist)))\n",
        "\n",
        "    save_path = saver.save(sess, model_path)\n",
        "    print(\"Model saved in file: \",save_path)\n",
        "\n",
        "\n",
        "    rlist=[]\n",
        "    recent_rlist=[]\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver.restore(sess, model_path)\n",
        "\n",
        "    print(\"Model restored form file: \", save_path)\n",
        "    for episode in range(10):\n",
        "        # state 초기화\n",
        "        s = env.reset()\n",
        "\n",
        "        rall = 0\n",
        "        d = False\n",
        "        count = 0\n",
        "        # 에피소드가 끝나기 전까지 반복\n",
        "        while not d :\n",
        "           # env.render()\n",
        "            count += 1\n",
        "            # state 값의 전처리\n",
        "            s_t = np.reshape(s, [1, INPUT])\n",
        "\n",
        "            # 현재 상태의 Q값을 에측\n",
        "            Q = sess.run(Q_pre, feed_dict={x: s_t})\n",
        "            a = np.argmax(Q)\n",
        "\n",
        "            # 결정된 action으로 Environment에 입력\n",
        "            s, r, d, _ = env.step(a)\n",
        "\n",
        "            # 총 reward 합\n",
        "            rall += r\n",
        "\n",
        "\n",
        "        rlist.append(rall)\n",
        "\n",
        "        print(\"Episode : {} steps : {} r={}. averge reward : {}\".format(episode, count, rall,\n",
        "                                                                        np.mean(rlist)))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "초기시작시 Wm과 Wt확인\n",
            "[[ 0.89659953  0.08986282]\n",
            " [-0.9402487  -0.43435168]\n",
            " [ 0.5121553  -0.33361053]\n",
            " [-0.9986341   0.3573928 ]]\n",
            "[[ 0.89659953  0.08986282]\n",
            " [-0.9402487  -0.43435168]\n",
            " [ 0.5121553  -0.33361053]\n",
            " [-0.9986341   0.3573928 ]]\n",
            "recent_rlist\n",
            "[0]\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "Episode:1 steps:13 reward:13.0 average reward:6.5 recent reward:6.5\n",
            "recent_rlist\n",
            "[0, 13.0]\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리버퍼에서 꺼내온 샘플 Target Str At Rt St+1 done count_R\n",
            "[array([[-0.03381051, -1.01279127,  0.07734901,  1.3931809 ]]), 0, 1.0, array([-0.05406633, -1.20878614,  0.10521263,  1.70901214]), False, 28]\n",
            "메모리버퍼에서 꺼내온 St로 Q(St,a)를 예측 Qpre=st*Wm\n",
            "[[ 0.89659953  0.08986282]\n",
            " [-0.9402487  -0.43435168]\n",
            " [ 0.5121553  -0.33361053]\n",
            " [-0.9986341   0.3573928 ]]\n",
            "y(st.a)에 사용된 Wt\n",
            "[[ 0.89659953  0.08986282]\n",
            " [-0.9402487  -0.43435168]\n",
            " [ 0.5121553  -0.33361053]\n",
            " [-0.9986341   0.3573928 ]]\n",
            "표적 Q(st+1,at)==>y(st,a)=st+1*Wt, Q1 및 Q\n",
            "[[-0.5647088  1.0958683]] [[1.0958683 0.9089776]]\n",
            "훈련에참여한 값 St[[-0.03381051 -1.01279127  0.07734901  1.3931809 ]]\n",
            "훈련에참여한 값 Target[[1.0958683 0.9089776]]\n",
            " REPLY 훈련W값\n",
            "[array([[ 0.89635354,  0.08986282],\n",
            "       [-0.9411559 , -0.43435168],\n",
            "       [ 0.51258266, -0.33361053],\n",
            "       [-0.9977033 ,  0.3573928 ]], dtype=float32)]\n",
            "최종 loss2.327364683151245\n",
            "메모리버퍼에서 꺼내온 샘플 Target Str At Rt St+1 done count_R\n",
            "[array([[-0.10634102, -1.60120639,  0.18004295,  2.36488441]]), 1, 1.0, array([-0.13836515, -1.40808713,  0.22734064,  2.13253502]), True, 31]\n",
            "메모리버퍼에서 꺼내온 St로 Q(St,a)를 예측 Qpre=st*Wm\n",
            "[[ 0.89635354  0.08986282]\n",
            " [-0.9411559  -0.43435168]\n",
            " [ 0.51258266 -0.33361053]\n",
            " [-0.9977033   0.3573928 ]]\n",
            "훈련에참여한 값 St[[-0.10634102 -1.60120639  0.18004295  2.36488441]]\n",
            "훈련에참여한 값 Target[[  -0.85550034 -100.        ]]\n",
            " REPLY 훈련W값\n",
            "[array([[ 0.8961888 ,  0.09059621],\n",
            "       [-0.94176376, -0.43360826],\n",
            "       [ 0.51286894, -0.3343483 ],\n",
            "       [-0.9970797 ,  0.35664913]], dtype=float32)]\n",
            "최종 loss10296.3759765625\n",
            "Wm==>Wt로 복사\n",
            "[[ 0.8961888   0.09059621]\n",
            " [-0.94176376 -0.43360826]\n",
            " [ 0.51286894 -0.3343483 ]\n",
            " [-0.9970797   0.35664913]]\n",
            "Episode:2 steps:31 reward:31.0 average reward:14.666666666666666 recent reward:14.666666666666666\n",
            "Model saved in file:  save/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from save/model.ckpt\n",
            "Model restored form file:  save/model.ckpt\n",
            "Episode : 0 steps : 132 r=132.0. averge reward : 132.0\n",
            "Episode : 1 steps : 112 r=112.0. averge reward : 122.0\n",
            "Episode : 2 steps : 157 r=157.0. averge reward : 133.66666666666666\n",
            "Episode : 3 steps : 148 r=148.0. averge reward : 137.25\n",
            "Episode : 4 steps : 128 r=128.0. averge reward : 135.4\n",
            "Episode : 5 steps : 121 r=121.0. averge reward : 133.0\n",
            "Episode : 6 steps : 167 r=167.0. averge reward : 137.85714285714286\n",
            "Episode : 7 steps : 114 r=114.0. averge reward : 134.875\n",
            "Episode : 8 steps : 101 r=101.0. averge reward : 131.11111111111111\n",
            "Episode : 9 steps : 121 r=121.0. averge reward : 130.1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
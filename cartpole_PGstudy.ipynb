{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMnJzd2KyQhA+IVWCw7cjX2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yougoal/leejunghoon/blob/master/cartpole_PGstudy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFUkibfH79Oe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56b0ba3e-059b-434f-d9bd-f3895a39824b"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from collections import deque\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# 하이퍼 파라미터\n",
        "LEARNING_RATE = 0.005\n",
        "INPUT = env.observation_space.shape[0]\n",
        "OUTPUT = env.action_space.n\n",
        "DISCOUNT = 0.99\n",
        "\n",
        "\n",
        "def discount_rewards(r):\n",
        "    '''Discounted reward를 구하기 위한 함수\n",
        "    \n",
        "    Args:\n",
        "         r(np.array): reward 값이 저장된 array\n",
        "    \n",
        "    Returns:\n",
        "        discounted_r(np.array): Discounted 된 reward가 저장된 array\n",
        "    '''\n",
        "    discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(len(r))):\n",
        "        running_add = running_add * DISCOUNT + r[t]\n",
        "        discounted_r[t] = running_add\n",
        "\n",
        "    return discounted_r\n",
        "\n",
        "\n",
        "def train_episodic(PGagent, x, y, adv):\n",
        "    '''에피소드당 학습을 하기위한 함수\n",
        "    \n",
        "    Args:\n",
        "        PGagent(PolicyGradient): 학습될 네트워크\n",
        "        x(np.array): State가 저장되어있는 array\n",
        "        y(np.array): Action(one_hot)이 저장되어있는 array\n",
        "        adv(np.array) : Discounted reward가 저장되어있는 array\n",
        "        \n",
        "    Returns:\n",
        "        l(float): 네트워크에 의한 loss\n",
        "    '''\n",
        "    l,_ = PGagent.sess.run([PGagent.loss, PGagent.train], feed_dict={PGagent.X: x, PGagent.Y: y, PGagent.adv : adv})\n",
        "    return l\n",
        "\n",
        "def play_cartpole(PGagent):\n",
        "    '''학습된 네트워크로 Play하기 위한 함수\n",
        "    \n",
        "    Args:\n",
        "         PGagent(PolicyGradient): 학습된 네트워크\n",
        "    '''\n",
        "    print(\"Play Cartpole!\")\n",
        "    episode = 0\n",
        "    while True:\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        rall = 0\n",
        "        episode += 1\n",
        "        while not done:\n",
        "            env.render()\n",
        "            action_p = PGagent.sess.run(PGagent.a_pre, feed_dict={PGagent.X : s})\n",
        "            s1, reward, done, _ = env.step(np.argmax(action_p))\n",
        "            s = s1\n",
        "            rall += reward\n",
        "        print(\"[Episode {0:6f}] Reward: {1:4f} \".format(episode, rall))\n",
        "\n",
        "class PolicyGradient:\n",
        "    def __init__(self, sess, input_size, output_size):\n",
        "        self.sess = sess\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.build_network()\n",
        "\n",
        "    def build_network(self):\n",
        "        self.X = tf.placeholder('float',[None, self.input_size])\n",
        "        self.Y = tf.placeholder('float', [None, self.output_size])\n",
        "        self.adv = tf.placeholder('float')\n",
        "\n",
        "        w1 = tf.get_variable('w1', shape=[self.input_size, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
        "        w2 = tf.get_variable('w2', shape=[128, self.output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "        l1 = tf.nn.relu(tf.matmul(self.X, w1))\n",
        "        self.a_pre = tf.nn.softmax(tf.matmul(l1,w2))\n",
        "\n",
        "        self.log_p = self.Y * tf.log(self.a_pre)\n",
        "        self.log_lik = self.log_p * self.adv\n",
        "        self.loss = tf.reduce_mean(tf.reduce_sum(-self.log_lik, axis=1))\n",
        "        self.train = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state_t = np.reshape(state, [1, self.input_size])\n",
        "        action_p = self.sess.run(self.a_pre, feed_dict={self.X : state_t})\n",
        "\n",
        "        # 각 액션의 확률로 액션을 결정\n",
        "        action = np.random.choice(np.arange(self.output_size), p=action_p[0])\n",
        "\n",
        "        return action\n",
        "\n",
        "def main():\n",
        "    with tf.Session() as sess:\n",
        "        PGagent = PolicyGradient(sess, INPUT, OUTPUT)\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        episode = 0\n",
        "        recent_rlist = deque(maxlen=100)\n",
        "        recent_rlist.append(0)\n",
        "\n",
        "        # 최근 100개의 점수가 195점 넘을 때까지 학습\n",
        "        while np.mean(recent_rlist) <= 195:\n",
        "            episode += 1\n",
        "            episode_memory = deque()\n",
        "            rall = 0\n",
        "            s = env.reset()\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                # 액션 선택\n",
        "                action = PGagent.get_action(s)\n",
        "\n",
        "                # action을 one_hot으로 표현\n",
        "                y = np.zeros(OUTPUT)\n",
        "                y[action] = 1\n",
        "\n",
        "                s1, reward, done, _ = env.step(action)\n",
        "                rall += reward\n",
        "\n",
        "                # 에피소드 메모리에 저장\n",
        "                episode_memory.append([s, y, reward])\n",
        "                s = s1\n",
        "\n",
        "                # 에피소드가 끝났을때 학습\n",
        "                if done:\n",
        "                    episode_memory = np.array(episode_memory)\n",
        "\n",
        "                    discounted_rewards = discount_rewards(np.vstack(episode_memory[:,2]))\n",
        "\n",
        "                    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() +\n",
        "                                                                                             1e-7)\n",
        "\n",
        "                    l = train_episodic(PGagent, np.vstack(episode_memory[:,0]), np.vstack(episode_memory[:,1]),\n",
        "                                      discounted_rewards)\n",
        "\n",
        "                    recent_rlist.append(rall)\n",
        "\n",
        "            print(\"[Episode {0:6f}] Reward: {1:4f} Loss: {2:5.5f} Recent Reward: {3:4f}\".format(episode, rall, l,\n",
        "                                                                                                np.mean(recent_rlist)))\n",
        "\n",
        "        play_cartpole(PGagent)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "[Episode 1.000000] Reward: 20.000000 Loss: -0.00272 Recent Reward: 10.000000\n",
            "[Episode 2.000000] Reward: 18.000000 Loss: -0.03730 Recent Reward: 12.666667\n",
            "[Episode 3.000000] Reward: 15.000000 Loss: -0.00057 Recent Reward: 13.250000\n",
            "[Episode 4.000000] Reward: 14.000000 Loss: 0.00619 Recent Reward: 13.400000\n",
            "[Episode 5.000000] Reward: 21.000000 Loss: -0.01443 Recent Reward: 14.666667\n",
            "[Episode 6.000000] Reward: 14.000000 Loss: 0.03376 Recent Reward: 14.571429\n",
            "[Episode 7.000000] Reward: 18.000000 Loss: 0.02926 Recent Reward: 15.000000\n",
            "[Episode 8.000000] Reward: 18.000000 Loss: -0.01044 Recent Reward: 15.333333\n",
            "[Episode 9.000000] Reward: 14.000000 Loss: 0.08110 Recent Reward: 15.200000\n",
            "[Episode 10.000000] Reward: 37.000000 Loss: -0.00545 Recent Reward: 17.181818\n",
            "[Episode 11.000000] Reward: 20.000000 Loss: 0.01817 Recent Reward: 17.416667\n",
            "[Episode 12.000000] Reward: 26.000000 Loss: 0.01298 Recent Reward: 18.076923\n",
            "[Episode 13.000000] Reward: 44.000000 Loss: -0.01589 Recent Reward: 19.928571\n",
            "[Episode 14.000000] Reward: 16.000000 Loss: 0.00226 Recent Reward: 19.666667\n",
            "[Episode 15.000000] Reward: 28.000000 Loss: -0.03220 Recent Reward: 20.187500\n",
            "[Episode 16.000000] Reward: 24.000000 Loss: 0.00057 Recent Reward: 20.411765\n",
            "[Episode 17.000000] Reward: 55.000000 Loss: 0.00532 Recent Reward: 22.333333\n",
            "[Episode 18.000000] Reward: 50.000000 Loss: -0.03143 Recent Reward: 23.789474\n",
            "[Episode 19.000000] Reward: 13.000000 Loss: 0.02412 Recent Reward: 23.250000\n",
            "[Episode 20.000000] Reward: 41.000000 Loss: 0.01612 Recent Reward: 24.095238\n",
            "[Episode 21.000000] Reward: 52.000000 Loss: 0.00672 Recent Reward: 25.363636\n",
            "[Episode 22.000000] Reward: 43.000000 Loss: -0.03131 Recent Reward: 26.130435\n",
            "[Episode 23.000000] Reward: 25.000000 Loss: -0.02554 Recent Reward: 26.083333\n",
            "[Episode 24.000000] Reward: 53.000000 Loss: -0.03342 Recent Reward: 27.160000\n",
            "[Episode 25.000000] Reward: 53.000000 Loss: 0.01202 Recent Reward: 28.153846\n",
            "[Episode 26.000000] Reward: 50.000000 Loss: 0.01044 Recent Reward: 28.962963\n",
            "[Episode 27.000000] Reward: 22.000000 Loss: 0.02496 Recent Reward: 28.714286\n",
            "[Episode 28.000000] Reward: 44.000000 Loss: 0.02656 Recent Reward: 29.241379\n",
            "[Episode 29.000000] Reward: 46.000000 Loss: 0.00241 Recent Reward: 29.800000\n",
            "[Episode 30.000000] Reward: 13.000000 Loss: 0.08859 Recent Reward: 29.258065\n",
            "[Episode 31.000000] Reward: 52.000000 Loss: 0.00568 Recent Reward: 29.968750\n",
            "[Episode 32.000000] Reward: 15.000000 Loss: 0.09520 Recent Reward: 29.515152\n",
            "[Episode 33.000000] Reward: 28.000000 Loss: 0.00179 Recent Reward: 29.470588\n",
            "[Episode 34.000000] Reward: 85.000000 Loss: -0.00330 Recent Reward: 31.057143\n",
            "[Episode 35.000000] Reward: 40.000000 Loss: -0.00711 Recent Reward: 31.305556\n",
            "[Episode 36.000000] Reward: 51.000000 Loss: 0.01150 Recent Reward: 31.837838\n",
            "[Episode 37.000000] Reward: 17.000000 Loss: 0.00494 Recent Reward: 31.447368\n",
            "[Episode 38.000000] Reward: 110.000000 Loss: 0.00516 Recent Reward: 33.461538\n",
            "[Episode 39.000000] Reward: 44.000000 Loss: 0.00244 Recent Reward: 33.725000\n",
            "[Episode 40.000000] Reward: 27.000000 Loss: -0.06923 Recent Reward: 33.560976\n",
            "[Episode 41.000000] Reward: 15.000000 Loss: -0.11409 Recent Reward: 33.119048\n",
            "[Episode 42.000000] Reward: 45.000000 Loss: -0.02251 Recent Reward: 33.395349\n",
            "[Episode 43.000000] Reward: 66.000000 Loss: 0.01629 Recent Reward: 34.136364\n",
            "[Episode 44.000000] Reward: 45.000000 Loss: 0.00822 Recent Reward: 34.377778\n",
            "[Episode 45.000000] Reward: 38.000000 Loss: 0.00135 Recent Reward: 34.456522\n",
            "[Episode 46.000000] Reward: 52.000000 Loss: 0.00959 Recent Reward: 34.829787\n",
            "[Episode 47.000000] Reward: 73.000000 Loss: -0.05148 Recent Reward: 35.625000\n",
            "[Episode 48.000000] Reward: 45.000000 Loss: -0.00113 Recent Reward: 35.816327\n",
            "[Episode 49.000000] Reward: 33.000000 Loss: -0.10104 Recent Reward: 35.760000\n",
            "[Episode 50.000000] Reward: 40.000000 Loss: 0.01568 Recent Reward: 35.843137\n",
            "[Episode 51.000000] Reward: 41.000000 Loss: 0.00687 Recent Reward: 35.942308\n",
            "[Episode 52.000000] Reward: 88.000000 Loss: -0.00461 Recent Reward: 36.924528\n",
            "[Episode 53.000000] Reward: 75.000000 Loss: -0.01952 Recent Reward: 37.629630\n",
            "[Episode 54.000000] Reward: 50.000000 Loss: -0.00035 Recent Reward: 37.854545\n",
            "[Episode 55.000000] Reward: 29.000000 Loss: -0.07101 Recent Reward: 37.696429\n",
            "[Episode 56.000000] Reward: 44.000000 Loss: -0.01355 Recent Reward: 37.807018\n",
            "[Episode 57.000000] Reward: 28.000000 Loss: -0.14843 Recent Reward: 37.637931\n",
            "[Episode 58.000000] Reward: 29.000000 Loss: -0.14263 Recent Reward: 37.491525\n",
            "[Episode 59.000000] Reward: 28.000000 Loss: -0.07415 Recent Reward: 37.333333\n",
            "[Episode 60.000000] Reward: 109.000000 Loss: 0.00616 Recent Reward: 38.508197\n",
            "[Episode 61.000000] Reward: 85.000000 Loss: -0.00473 Recent Reward: 39.258065\n",
            "[Episode 62.000000] Reward: 55.000000 Loss: 0.01106 Recent Reward: 39.507937\n",
            "[Episode 63.000000] Reward: 54.000000 Loss: 0.07216 Recent Reward: 39.734375\n",
            "[Episode 64.000000] Reward: 54.000000 Loss: 0.01632 Recent Reward: 39.953846\n",
            "[Episode 65.000000] Reward: 36.000000 Loss: 0.00604 Recent Reward: 39.893939\n",
            "[Episode 66.000000] Reward: 36.000000 Loss: -0.04390 Recent Reward: 39.835821\n",
            "[Episode 67.000000] Reward: 59.000000 Loss: -0.07369 Recent Reward: 40.117647\n",
            "[Episode 68.000000] Reward: 24.000000 Loss: -0.04317 Recent Reward: 39.884058\n",
            "[Episode 69.000000] Reward: 65.000000 Loss: 0.04239 Recent Reward: 40.242857\n",
            "[Episode 70.000000] Reward: 69.000000 Loss: 0.00925 Recent Reward: 40.647887\n",
            "[Episode 71.000000] Reward: 75.000000 Loss: -0.01382 Recent Reward: 41.125000\n",
            "[Episode 72.000000] Reward: 57.000000 Loss: 0.00050 Recent Reward: 41.342466\n",
            "[Episode 73.000000] Reward: 98.000000 Loss: 0.00368 Recent Reward: 42.108108\n",
            "[Episode 74.000000] Reward: 116.000000 Loss: 0.01582 Recent Reward: 43.093333\n",
            "[Episode 75.000000] Reward: 111.000000 Loss: 0.02716 Recent Reward: 43.986842\n",
            "[Episode 76.000000] Reward: 65.000000 Loss: 0.01752 Recent Reward: 44.259740\n",
            "[Episode 77.000000] Reward: 74.000000 Loss: -0.10544 Recent Reward: 44.641026\n",
            "[Episode 78.000000] Reward: 104.000000 Loss: -0.00536 Recent Reward: 45.392405\n",
            "[Episode 79.000000] Reward: 70.000000 Loss: -0.01465 Recent Reward: 45.700000\n",
            "[Episode 80.000000] Reward: 96.000000 Loss: 0.03679 Recent Reward: 46.320988\n",
            "[Episode 81.000000] Reward: 177.000000 Loss: -0.00879 Recent Reward: 47.914634\n",
            "[Episode 82.000000] Reward: 82.000000 Loss: 0.00682 Recent Reward: 48.325301\n",
            "[Episode 83.000000] Reward: 29.000000 Loss: 0.04516 Recent Reward: 48.095238\n",
            "[Episode 84.000000] Reward: 141.000000 Loss: -0.00859 Recent Reward: 49.188235\n",
            "[Episode 85.000000] Reward: 200.000000 Loss: 0.00210 Recent Reward: 50.941860\n",
            "[Episode 86.000000] Reward: 110.000000 Loss: 0.00845 Recent Reward: 51.620690\n",
            "[Episode 87.000000] Reward: 200.000000 Loss: -0.00046 Recent Reward: 53.306818\n",
            "[Episode 88.000000] Reward: 200.000000 Loss: 0.00314 Recent Reward: 54.955056\n",
            "[Episode 89.000000] Reward: 133.000000 Loss: 0.01266 Recent Reward: 55.822222\n",
            "[Episode 90.000000] Reward: 200.000000 Loss: 0.00547 Recent Reward: 57.406593\n",
            "[Episode 91.000000] Reward: 107.000000 Loss: -0.01254 Recent Reward: 57.945652\n",
            "[Episode 92.000000] Reward: 200.000000 Loss: -0.00104 Recent Reward: 59.473118\n",
            "[Episode 93.000000] Reward: 200.000000 Loss: -0.00093 Recent Reward: 60.968085\n",
            "[Episode 94.000000] Reward: 93.000000 Loss: -0.10495 Recent Reward: 61.305263\n",
            "[Episode 95.000000] Reward: 136.000000 Loss: -0.03857 Recent Reward: 62.083333\n",
            "[Episode 96.000000] Reward: 189.000000 Loss: 0.01697 Recent Reward: 63.391753\n",
            "[Episode 97.000000] Reward: 200.000000 Loss: 0.03283 Recent Reward: 64.785714\n",
            "[Episode 98.000000] Reward: 120.000000 Loss: -0.00769 Recent Reward: 65.343434\n",
            "[Episode 99.000000] Reward: 112.000000 Loss: -0.01897 Recent Reward: 65.810000\n",
            "[Episode 100.000000] Reward: 146.000000 Loss: -0.01673 Recent Reward: 67.270000\n",
            "[Episode 101.000000] Reward: 200.000000 Loss: -0.01033 Recent Reward: 69.070000\n",
            "[Episode 102.000000] Reward: 106.000000 Loss: 0.01517 Recent Reward: 69.950000\n",
            "[Episode 103.000000] Reward: 200.000000 Loss: -0.00714 Recent Reward: 71.800000\n",
            "[Episode 104.000000] Reward: 197.000000 Loss: 0.00385 Recent Reward: 73.630000\n",
            "[Episode 105.000000] Reward: 147.000000 Loss: 0.00062 Recent Reward: 74.890000\n",
            "[Episode 106.000000] Reward: 149.000000 Loss: -0.00861 Recent Reward: 76.240000\n",
            "[Episode 107.000000] Reward: 186.000000 Loss: 0.01141 Recent Reward: 77.920000\n",
            "[Episode 108.000000] Reward: 88.000000 Loss: -0.01005 Recent Reward: 78.620000\n",
            "[Episode 109.000000] Reward: 200.000000 Loss: 0.00294 Recent Reward: 80.480000\n",
            "[Episode 110.000000] Reward: 200.000000 Loss: 0.01655 Recent Reward: 82.110000\n",
            "[Episode 111.000000] Reward: 200.000000 Loss: -0.00105 Recent Reward: 83.910000\n",
            "[Episode 112.000000] Reward: 200.000000 Loss: 0.00730 Recent Reward: 85.650000\n",
            "[Episode 113.000000] Reward: 200.000000 Loss: -0.02251 Recent Reward: 87.210000\n",
            "[Episode 114.000000] Reward: 140.000000 Loss: -0.02017 Recent Reward: 88.450000\n",
            "[Episode 115.000000] Reward: 200.000000 Loss: 0.00851 Recent Reward: 90.170000\n",
            "[Episode 116.000000] Reward: 31.000000 Loss: -0.17953 Recent Reward: 90.240000\n",
            "[Episode 117.000000] Reward: 200.000000 Loss: 0.01997 Recent Reward: 91.690000\n",
            "[Episode 118.000000] Reward: 192.000000 Loss: -0.03487 Recent Reward: 93.110000\n",
            "[Episode 119.000000] Reward: 196.000000 Loss: 0.03795 Recent Reward: 94.940000\n",
            "[Episode 120.000000] Reward: 200.000000 Loss: -0.02674 Recent Reward: 96.530000\n",
            "[Episode 121.000000] Reward: 128.000000 Loss: 0.04285 Recent Reward: 97.290000\n",
            "[Episode 122.000000] Reward: 121.000000 Loss: -0.01554 Recent Reward: 98.070000\n",
            "[Episode 123.000000] Reward: 200.000000 Loss: -0.00123 Recent Reward: 99.820000\n",
            "[Episode 124.000000] Reward: 128.000000 Loss: 0.02983 Recent Reward: 100.570000\n",
            "[Episode 125.000000] Reward: 110.000000 Loss: -0.00518 Recent Reward: 101.140000\n",
            "[Episode 126.000000] Reward: 200.000000 Loss: 0.02757 Recent Reward: 102.640000\n",
            "[Episode 127.000000] Reward: 163.000000 Loss: 0.00781 Recent Reward: 104.050000\n",
            "[Episode 128.000000] Reward: 97.000000 Loss: 0.02836 Recent Reward: 104.580000\n",
            "[Episode 129.000000] Reward: 150.000000 Loss: 0.03584 Recent Reward: 105.620000\n",
            "[Episode 130.000000] Reward: 200.000000 Loss: 0.02069 Recent Reward: 107.490000\n",
            "[Episode 131.000000] Reward: 87.000000 Loss: -0.03309 Recent Reward: 107.840000\n",
            "[Episode 132.000000] Reward: 125.000000 Loss: -0.00325 Recent Reward: 108.940000\n",
            "[Episode 133.000000] Reward: 200.000000 Loss: -0.00344 Recent Reward: 110.660000\n",
            "[Episode 134.000000] Reward: 199.000000 Loss: 0.00372 Recent Reward: 111.800000\n",
            "[Episode 135.000000] Reward: 79.000000 Loss: 0.05077 Recent Reward: 112.190000\n",
            "[Episode 136.000000] Reward: 200.000000 Loss: 0.00059 Recent Reward: 113.680000\n",
            "[Episode 137.000000] Reward: 199.000000 Loss: 0.02767 Recent Reward: 115.500000\n",
            "[Episode 138.000000] Reward: 122.000000 Loss: -0.01346 Recent Reward: 115.620000\n",
            "[Episode 139.000000] Reward: 66.000000 Loss: 0.01183 Recent Reward: 115.840000\n",
            "[Episode 140.000000] Reward: 35.000000 Loss: -0.04460 Recent Reward: 115.920000\n",
            "[Episode 141.000000] Reward: 73.000000 Loss: -0.07378 Recent Reward: 116.500000\n",
            "[Episode 142.000000] Reward: 200.000000 Loss: -0.01908 Recent Reward: 118.050000\n",
            "[Episode 143.000000] Reward: 69.000000 Loss: 0.08070 Recent Reward: 118.080000\n",
            "[Episode 144.000000] Reward: 200.000000 Loss: -0.00641 Recent Reward: 119.630000\n",
            "[Episode 145.000000] Reward: 88.000000 Loss: -0.00264 Recent Reward: 120.130000\n",
            "[Episode 146.000000] Reward: 152.000000 Loss: 0.01653 Recent Reward: 121.130000\n",
            "[Episode 147.000000] Reward: 173.000000 Loss: 0.03289 Recent Reward: 122.130000\n",
            "[Episode 148.000000] Reward: 105.000000 Loss: 0.00426 Recent Reward: 122.730000\n",
            "[Episode 149.000000] Reward: 58.000000 Loss: -0.00084 Recent Reward: 122.980000\n",
            "[Episode 150.000000] Reward: 200.000000 Loss: 0.00225 Recent Reward: 124.580000\n",
            "[Episode 151.000000] Reward: 181.000000 Loss: -0.03423 Recent Reward: 125.980000\n",
            "[Episode 152.000000] Reward: 200.000000 Loss: -0.01558 Recent Reward: 127.100000\n",
            "[Episode 153.000000] Reward: 197.000000 Loss: -0.00775 Recent Reward: 128.320000\n",
            "[Episode 154.000000] Reward: 200.000000 Loss: -0.03896 Recent Reward: 129.820000\n",
            "[Episode 155.000000] Reward: 83.000000 Loss: -0.00384 Recent Reward: 130.360000\n",
            "[Episode 156.000000] Reward: 149.000000 Loss: 0.02686 Recent Reward: 131.410000\n",
            "[Episode 157.000000] Reward: 67.000000 Loss: -0.01401 Recent Reward: 131.800000\n",
            "[Episode 158.000000] Reward: 84.000000 Loss: -0.04406 Recent Reward: 132.350000\n",
            "[Episode 159.000000] Reward: 108.000000 Loss: 0.01401 Recent Reward: 133.150000\n",
            "[Episode 160.000000] Reward: 141.000000 Loss: -0.14323 Recent Reward: 133.470000\n",
            "[Episode 161.000000] Reward: 200.000000 Loss: 0.01374 Recent Reward: 134.620000\n",
            "[Episode 162.000000] Reward: 102.000000 Loss: 0.00513 Recent Reward: 135.090000\n",
            "[Episode 163.000000] Reward: 200.000000 Loss: 0.00094 Recent Reward: 136.550000\n",
            "[Episode 164.000000] Reward: 26.000000 Loss: -0.07697 Recent Reward: 136.270000\n",
            "[Episode 165.000000] Reward: 99.000000 Loss: -0.01872 Recent Reward: 136.900000\n",
            "[Episode 166.000000] Reward: 101.000000 Loss: -0.01290 Recent Reward: 137.550000\n",
            "[Episode 167.000000] Reward: 143.000000 Loss: 0.02915 Recent Reward: 138.390000\n",
            "[Episode 168.000000] Reward: 142.000000 Loss: 0.01387 Recent Reward: 139.570000\n",
            "[Episode 169.000000] Reward: 163.000000 Loss: -0.05835 Recent Reward: 140.550000\n",
            "[Episode 170.000000] Reward: 200.000000 Loss: 0.04195 Recent Reward: 141.860000\n",
            "[Episode 171.000000] Reward: 96.000000 Loss: 0.06162 Recent Reward: 142.070000\n",
            "[Episode 172.000000] Reward: 127.000000 Loss: 0.00491 Recent Reward: 142.770000\n",
            "[Episode 173.000000] Reward: 97.000000 Loss: 0.02568 Recent Reward: 142.760000\n",
            "[Episode 174.000000] Reward: 192.000000 Loss: 0.00882 Recent Reward: 143.520000\n",
            "[Episode 175.000000] Reward: 76.000000 Loss: 0.02618 Recent Reward: 143.170000\n",
            "[Episode 176.000000] Reward: 96.000000 Loss: 0.04078 Recent Reward: 143.480000\n",
            "[Episode 177.000000] Reward: 179.000000 Loss: -0.00745 Recent Reward: 144.530000\n",
            "[Episode 178.000000] Reward: 200.000000 Loss: 0.01533 Recent Reward: 145.490000\n",
            "[Episode 179.000000] Reward: 177.000000 Loss: 0.03342 Recent Reward: 146.560000\n",
            "[Episode 180.000000] Reward: 200.000000 Loss: 0.00909 Recent Reward: 147.600000\n",
            "[Episode 181.000000] Reward: 187.000000 Loss: 0.03777 Recent Reward: 147.700000\n",
            "[Episode 182.000000] Reward: 115.000000 Loss: -0.00284 Recent Reward: 148.030000\n",
            "[Episode 183.000000] Reward: 182.000000 Loss: -0.00491 Recent Reward: 149.560000\n",
            "[Episode 184.000000] Reward: 200.000000 Loss: -0.01756 Recent Reward: 150.150000\n",
            "[Episode 185.000000] Reward: 84.000000 Loss: -0.03034 Recent Reward: 148.990000\n",
            "[Episode 186.000000] Reward: 200.000000 Loss: -0.02085 Recent Reward: 149.890000\n",
            "[Episode 187.000000] Reward: 200.000000 Loss: 0.01133 Recent Reward: 149.890000\n",
            "[Episode 188.000000] Reward: 130.000000 Loss: -0.02609 Recent Reward: 149.190000\n",
            "[Episode 189.000000] Reward: 76.000000 Loss: 0.03662 Recent Reward: 148.620000\n",
            "[Episode 190.000000] Reward: 200.000000 Loss: 0.00566 Recent Reward: 148.620000\n",
            "[Episode 191.000000] Reward: 200.000000 Loss: 0.04219 Recent Reward: 149.550000\n",
            "[Episode 192.000000] Reward: 200.000000 Loss: -0.00138 Recent Reward: 149.550000\n",
            "[Episode 193.000000] Reward: 200.000000 Loss: -0.03087 Recent Reward: 149.550000\n",
            "[Episode 194.000000] Reward: 171.000000 Loss: 0.03821 Recent Reward: 150.330000\n",
            "[Episode 195.000000] Reward: 200.000000 Loss: 0.00161 Recent Reward: 150.970000\n",
            "[Episode 196.000000] Reward: 200.000000 Loss: -0.03145 Recent Reward: 151.080000\n",
            "[Episode 197.000000] Reward: 89.000000 Loss: 0.05458 Recent Reward: 149.970000\n",
            "[Episode 198.000000] Reward: 120.000000 Loss: -0.08099 Recent Reward: 149.970000\n",
            "[Episode 199.000000] Reward: 119.000000 Loss: 0.04625 Recent Reward: 150.040000\n",
            "[Episode 200.000000] Reward: 200.000000 Loss: 0.02051 Recent Reward: 150.580000\n",
            "[Episode 201.000000] Reward: 200.000000 Loss: 0.02173 Recent Reward: 150.580000\n",
            "[Episode 202.000000] Reward: 100.000000 Loss: 0.05548 Recent Reward: 150.520000\n",
            "[Episode 203.000000] Reward: 200.000000 Loss: -0.02549 Recent Reward: 150.520000\n",
            "[Episode 204.000000] Reward: 153.000000 Loss: -0.03173 Recent Reward: 150.080000\n",
            "[Episode 205.000000] Reward: 183.000000 Loss: -0.00841 Recent Reward: 150.440000\n",
            "[Episode 206.000000] Reward: 167.000000 Loss: 0.03690 Recent Reward: 150.620000\n",
            "[Episode 207.000000] Reward: 200.000000 Loss: -0.03570 Recent Reward: 150.760000\n",
            "[Episode 208.000000] Reward: 132.000000 Loss: 0.01505 Recent Reward: 151.200000\n",
            "[Episode 209.000000] Reward: 200.000000 Loss: -0.00372 Recent Reward: 151.200000\n",
            "[Episode 210.000000] Reward: 141.000000 Loss: 0.03512 Recent Reward: 150.610000\n",
            "[Episode 211.000000] Reward: 169.000000 Loss: 0.05660 Recent Reward: 150.300000\n",
            "[Episode 212.000000] Reward: 183.000000 Loss: 0.00241 Recent Reward: 150.130000\n",
            "[Episode 213.000000] Reward: 151.000000 Loss: -0.00625 Recent Reward: 149.640000\n",
            "[Episode 214.000000] Reward: 200.000000 Loss: -0.01042 Recent Reward: 150.240000\n",
            "[Episode 215.000000] Reward: 200.000000 Loss: 0.00434 Recent Reward: 150.240000\n",
            "[Episode 216.000000] Reward: 200.000000 Loss: 0.06023 Recent Reward: 151.930000\n",
            "[Episode 217.000000] Reward: 200.000000 Loss: -0.01015 Recent Reward: 151.930000\n",
            "[Episode 218.000000] Reward: 200.000000 Loss: -0.00800 Recent Reward: 152.010000\n",
            "[Episode 219.000000] Reward: 200.000000 Loss: 0.03507 Recent Reward: 152.050000\n",
            "[Episode 220.000000] Reward: 200.000000 Loss: 0.01129 Recent Reward: 152.050000\n",
            "[Episode 221.000000] Reward: 200.000000 Loss: 0.02211 Recent Reward: 152.770000\n",
            "[Episode 222.000000] Reward: 182.000000 Loss: -0.00230 Recent Reward: 153.380000\n",
            "[Episode 223.000000] Reward: 200.000000 Loss: -0.01094 Recent Reward: 153.380000\n",
            "[Episode 224.000000] Reward: 200.000000 Loss: 0.00346 Recent Reward: 154.100000\n",
            "[Episode 225.000000] Reward: 200.000000 Loss: 0.00813 Recent Reward: 155.000000\n",
            "[Episode 226.000000] Reward: 200.000000 Loss: -0.00738 Recent Reward: 155.000000\n",
            "[Episode 227.000000] Reward: 181.000000 Loss: 0.02648 Recent Reward: 155.180000\n",
            "[Episode 228.000000] Reward: 200.000000 Loss: -0.00452 Recent Reward: 156.210000\n",
            "[Episode 229.000000] Reward: 200.000000 Loss: 0.00584 Recent Reward: 156.710000\n",
            "[Episode 230.000000] Reward: 200.000000 Loss: 0.02209 Recent Reward: 156.710000\n",
            "[Episode 231.000000] Reward: 200.000000 Loss: -0.00615 Recent Reward: 157.840000\n",
            "[Episode 232.000000] Reward: 200.000000 Loss: 0.00881 Recent Reward: 158.590000\n",
            "[Episode 233.000000] Reward: 186.000000 Loss: 0.03761 Recent Reward: 158.450000\n",
            "[Episode 234.000000] Reward: 200.000000 Loss: 0.01755 Recent Reward: 158.460000\n",
            "[Episode 235.000000] Reward: 200.000000 Loss: 0.01602 Recent Reward: 159.670000\n",
            "[Episode 236.000000] Reward: 200.000000 Loss: 0.05227 Recent Reward: 159.670000\n",
            "[Episode 237.000000] Reward: 200.000000 Loss: 0.00364 Recent Reward: 159.680000\n",
            "[Episode 238.000000] Reward: 200.000000 Loss: 0.01964 Recent Reward: 160.460000\n",
            "[Episode 239.000000] Reward: 200.000000 Loss: 0.02229 Recent Reward: 161.800000\n",
            "[Episode 240.000000] Reward: 179.000000 Loss: 0.02153 Recent Reward: 163.240000\n",
            "[Episode 241.000000] Reward: 200.000000 Loss: -0.00980 Recent Reward: 164.510000\n",
            "[Episode 242.000000] Reward: 200.000000 Loss: -0.01393 Recent Reward: 164.510000\n",
            "[Episode 243.000000] Reward: 200.000000 Loss: 0.06454 Recent Reward: 165.820000\n",
            "[Episode 244.000000] Reward: 200.000000 Loss: 0.04568 Recent Reward: 165.820000\n",
            "[Episode 245.000000] Reward: 200.000000 Loss: 0.05742 Recent Reward: 166.940000\n",
            "[Episode 246.000000] Reward: 200.000000 Loss: -0.04568 Recent Reward: 167.420000\n",
            "[Episode 247.000000] Reward: 114.000000 Loss: 0.00366 Recent Reward: 166.830000\n",
            "[Episode 248.000000] Reward: 152.000000 Loss: 0.06966 Recent Reward: 167.300000\n",
            "[Episode 249.000000] Reward: 200.000000 Loss: -0.00352 Recent Reward: 168.720000\n",
            "[Episode 250.000000] Reward: 127.000000 Loss: 0.03676 Recent Reward: 167.990000\n",
            "[Episode 251.000000] Reward: 200.000000 Loss: -0.02388 Recent Reward: 168.180000\n",
            "[Episode 252.000000] Reward: 125.000000 Loss: -0.07483 Recent Reward: 167.430000\n",
            "[Episode 253.000000] Reward: 200.000000 Loss: 0.02620 Recent Reward: 167.460000\n",
            "[Episode 254.000000] Reward: 200.000000 Loss: 0.00701 Recent Reward: 167.460000\n",
            "[Episode 255.000000] Reward: 137.000000 Loss: 0.02647 Recent Reward: 168.000000\n",
            "[Episode 256.000000] Reward: 200.000000 Loss: 0.00747 Recent Reward: 168.510000\n",
            "[Episode 257.000000] Reward: 154.000000 Loss: 0.03444 Recent Reward: 169.380000\n",
            "[Episode 258.000000] Reward: 200.000000 Loss: 0.01794 Recent Reward: 170.540000\n",
            "[Episode 259.000000] Reward: 200.000000 Loss: 0.00003 Recent Reward: 171.460000\n",
            "[Episode 260.000000] Reward: 147.000000 Loss: 0.08942 Recent Reward: 171.520000\n",
            "[Episode 261.000000] Reward: 137.000000 Loss: 0.04557 Recent Reward: 170.890000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9b5232886792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-9b5232886792>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# 액션 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPGagent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# action을 one_hot으로 표현\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-9b5232886792>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mstate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0maction_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_pre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstate_t\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# 각 액션의 확률로 액션을 결정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHlY_BqY7-vP",
        "colab_type": "text"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from collections import deque\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "1.   항목 추가\n",
        "2.   항목 추가\n",
        "\n",
        "\n",
        "\n",
        "# 하이퍼 파라미터\n",
        "LEARNING_RATE = 0.005\n",
        "INPUT = env.observation_space.shape[0]\n",
        "OUTPUT = env.action_space.n\n",
        "DISCOUNT = 0.99\n",
        "\n",
        "\n",
        "def discount_rewards(r):\n",
        "    '''Discounted reward를 구하기 위한 함수\n",
        "    \n",
        "    Args:\n",
        "         r(np.array): reward 값이 저장된 array\n",
        "    \n",
        "    Returns:\n",
        "        discounted_r(np.array): Discounted 된 reward가 저장된 array\n",
        "    '''\n",
        "    discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(len(r))):\n",
        "        running_add = running_add * DISCOUNT + r[t]\n",
        "        discounted_r[t] = running_add\n",
        "\n",
        "    return discounted_r\n",
        "\n",
        "\n",
        "def train_episodic(PGagent, x, y, adv):\n",
        "    '''에피소드당 학습을 하기위한 함수\n",
        "    \n",
        "    Args:\n",
        "        PGagent(PolicyGradient): 학습될 네트워크\n",
        "        x(np.array): State가 저장되어있는 array\n",
        "        y(np.array): Action(one_hot)이 저장되어있는 array\n",
        "        adv(np.array) : Discounted reward가 저장되어있는 array\n",
        "        \n",
        "    Returns:\n",
        "        l(float): 네트워크에 의한 loss\n",
        "    '''\n",
        "    l,_ = PGagent.sess.run([PGagent.loss, PGagent.train], feed_dict={PGagent.X: x, PGagent.Y: y, PGagent.adv : adv})\n",
        "    return l\n",
        "\n",
        "def play_cartpole(PGagent):\n",
        "    '''학습된 네트워크로 Play하기 위한 함수\n",
        "    \n",
        "    Args:\n",
        "         PGagent(PolicyGradient): 학습된 네트워크\n",
        "    '''\n",
        "    print(\"Play Cartpole!\")\n",
        "    episode = 0\n",
        "    while True:\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        rall = 0\n",
        "        episode += 1\n",
        "        while not done:\n",
        "            env.render()\n",
        "            action_p = PGagent.sess.run(PGagent.a_pre, feed_dict={PGagent.X : s})\n",
        "            s1, reward, done, _ = env.step(np.argmax(action_p))\n",
        "            s = s1\n",
        "            rall += reward\n",
        "        print(\"[Episode {0:6f}] Reward: {1:4f} \".format(episode, rall))\n",
        "\n",
        "class PolicyGradient:\n",
        "    def __init__(self, sess, input_size, output_size):\n",
        "        self.sess = sess\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.build_network()\n",
        "\n",
        "    def build_network(self):\n",
        "        self.X = tf.placeholder('float',[None, self.input_size])\n",
        "        self.Y = tf.placeholder('float', [None, self.output_size])\n",
        "        self.adv = tf.placeholder('float')\n",
        "\n",
        "        w1 = tf.get_variable('w1', shape=[self.input_size, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
        "        w2 = tf.get_variable('w2', shape=[128, self.output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "        l1 = tf.nn.relu(tf.matmul(self.X, w1))\n",
        "        self.a_pre = tf.nn.softmax(tf.matmul(l1,w2))\n",
        "\n",
        "        self.log_p = self.Y * tf.log(self.a_pre)\n",
        "        self.log_lik = self.log_p * self.adv\n",
        "        self.loss = tf.reduce_mean(tf.reduce_sum(-self.log_lik, axis=1))\n",
        "        self.train = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state_t = np.reshape(state, [1, self.input_size])\n",
        "        action_p = self.sess.run(self.a_pre, feed_dict={self.X : state_t})\n",
        "\n",
        "        # 각 액션의 확률로 액션을 결정\n",
        "        action = np.random.choice(np.arange(self.output_size), p=action_p[0])\n",
        "\n",
        "        return action\n",
        "\n",
        "def main():\n",
        "    with tf.Session() as sess:\n",
        "        PGagent = PolicyGradient(sess, INPUT, OUTPUT)\n",
        "\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        episode = 0\n",
        "        recent_rlist = deque(maxlen=100)\n",
        "        recent_rlist.append(0)\n",
        "\n",
        "        # 최근 100개의 점수가 195점 넘을 때까지 학습\n",
        "        while np.mean(recent_rlist) <= 195:\n",
        "            episode += 1\n",
        "            episode_memory = deque()\n",
        "            rall = 0\n",
        "            s = env.reset()\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                # 액션 선택\n",
        "                action = PGagent.get_action(s)\n",
        "\n",
        "                # action을 one_hot으로 표현\n",
        "                y = np.zeros(OUTPUT)\n",
        "                y[action] = 1\n",
        "\n",
        "                s1, reward, done, _ = env.step(action)\n",
        "                rall += reward\n",
        "\n",
        "                # 에피소드 메모리에 저장\n",
        "                episode_memory.append([s, y, reward])\n",
        "                s = s1\n",
        "\n",
        "                # 에피소드가 끝났을때 학습\n",
        "                if done:\n",
        "                    episode_memory = np.array(episode_memory)\n",
        "\n",
        "                    discounted_rewards = discount_rewards(np.vstack(episode_memory[:,2]))\n",
        "\n",
        "                    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() +\n",
        "                                                                                             1e-7)\n",
        "\n",
        "                    l = train_episodic(PGagent, np.vstack(episode_memory[:,0]), np.vstack(episode_memory[:,1]),\n",
        "                                      discounted_rewards)\n",
        "\n",
        "                    recent_rlist.append(rall)\n",
        "\n",
        "            print(\"[Episode {0:6f}] Reward: {1:4f} Loss: {2:5.5f} Recent Reward: {3:4f}\".format(episode, rall, l,\n",
        "                                                                                                np.mean(recent_rlist)))\n",
        "\n",
        "        play_cartpole(PGagent)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}
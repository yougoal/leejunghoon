{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNVYOBtqMfAV6cR93VFQIB4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yougoal/leejunghoon/blob/master/cartpoleA2c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXK9S3LseqqQ",
        "colab_type": "code",
        "outputId": "4ee5fe53-1b7b-4fba-ac95-f639b0f58c4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from collections import deque\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# 하이퍼 파라미터\n",
        "LEARNING_RATE = 0.005\n",
        "INPUT = env.observation_space.shape[0]\n",
        "OUTPUT = env.action_space.n\n",
        "DISCOUNT = 0.99\n",
        "\n",
        "\n",
        "def discount_rewards(r):\n",
        "    '''Discounted reward를 구하기 위한 함수\n",
        "    Args:\n",
        "         r(np.array): reward 값이 저장된 array\n",
        "    Returns:\n",
        "        discounted_r(np.array): Discounted 된 reward가 저장된 array\n",
        "    '''\n",
        "    discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(len(r))):\n",
        "        running_add = running_add * DISCOUNT + r[t]\n",
        "        discounted_r[t] = running_add\n",
        "\n",
        "    return discounted_r\n",
        "def train_episodic(A2Cagent, x, y, r):\n",
        "    '''에피소드당 학습을 하기위한 함수\n",
        "    Args:\n",
        "        A2Cagent(ActorCritic): 학습될 네트워크\n",
        "        x(np.array): State가 저장되어있는 array\n",
        "        y(np.array): Action(one_hot)이 저장되어있는 array\n",
        "        r(np.array) : Discounted reward가 저장되어있는 array\n",
        "    Returns:\n",
        "        l(float): 네트워크에 의한 loss\n",
        "    '''\n",
        "    l, _ = A2Cagent.sess.run([A2Cagent.loss, A2Cagent.train], feed_dict={A2Cagent.X: x, A2Cagent.Y: y, A2Cagent.r: r})\n",
        "    return l\n",
        "\n",
        "def play_cartpole(A2Cagent):\n",
        "    '''학습된 네트워크로 Play하기 위한 함수\n",
        "    Args:\n",
        "         A2Cagent(ActorCritic): 학습된 네트워크\n",
        "    '''\n",
        "    print(\"Play Cartpole!\")\n",
        "    episode = 0\n",
        "    while True:\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        rall = 0\n",
        "        episode += 1\n",
        "        while not done:\n",
        "            env.render()\n",
        "            action_p = A2Cagent.get_action(s)\n",
        "            s1, reward, done, _ = env.step(action_p)\n",
        "            s = s1\n",
        "            rall += reward\n",
        "        print(\"[Episode {0:6f}] Reward: {1:4f} \".format(episode, rall))\n",
        "\n",
        "\n",
        "class ActorCritic:\n",
        "    def __init__(self, sess, input_size, output_size):\n",
        "        self.sess = sess\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.build_network()\n",
        "\n",
        "    def build_network(self):\n",
        "\n",
        "        self.X = tf.placeholder('float', [None, self.input_size])\n",
        "        self.Y = tf.placeholder('float', [None, self.output_size])\n",
        "\n",
        "        self.r = tf.placeholder('float')\n",
        "\n",
        "        # Actor Weight\n",
        "        w1_a = tf.get_variable('w1', shape=[self.input_size, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
        "        w2_a = tf.get_variable('w2', shape=[128, self.output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "        # Critic Weight\n",
        "        w1_c = tf.get_variable('w1_c', shape=[self.input_size, 128], initializer=tf.contrib.layers.xavier_initializer())\n",
        "        w2_c = tf.get_variable('w2_c', shape=[128, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "        # Actor Critic Network\n",
        "        l1_a = tf.nn.relu(tf.matmul(self.X, w1_a))\n",
        "        l1_c = tf.nn.relu(tf.matmul(self.X, w1_c))\n",
        "        self.a_prob = tf.nn.softmax(tf.matmul(l1_a, w2_a))\n",
        "        self.v = tf.matmul(l1_c, w2_c)\n",
        "\n",
        "        # A_t = R_t - V(S_t)\n",
        "        self.adv = self.r - self.v\n",
        "\n",
        "        # Policy loss\n",
        "        self.log_p = self.Y * tf.log(tf.clip_by_value(self.a_prob,1e-10,1.))\n",
        "        self.log_lik = self.log_p * tf.stop_gradient(self.adv)\n",
        "        self.p_loss = -tf.reduce_mean(tf.reduce_sum(self.log_lik, axis=1))\n",
        "\n",
        "        # entropy(for more exploration)\n",
        "        self.entropy = -tf.reduce_mean(tf.reduce_sum(self.a_prob * tf.log(tf.clip_by_value(self.a_prob,1e-10,1.)), axis=1))\n",
        "\n",
        "        # Value loss\n",
        "        self.v_loss = tf.reduce_mean(tf.square(self.v - self.r), axis=1)\n",
        "\n",
        "        # Total loss\n",
        "        self.loss = self.p_loss + self.v_loss - self.entropy * 0.01\n",
        "        self.train = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state_t = np.reshape(state, [1, self.input_size])\n",
        "        action_p = self.sess.run(self.a_prob, feed_dict={self.X: state_t})\n",
        "\n",
        "        # 각 액션의 확률로 액션을 결정\n",
        "        action = np.random.choice(np.arange(self.output_size), p=action_p[0])\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "def main():\n",
        "    with tf.Session() as sess:\n",
        "        A2Cagent = ActorCritic(sess, INPUT, OUTPUT)\n",
        "\n",
        "        A2Cagent.sess.run(tf.global_variables_initializer())\n",
        "        episode = 0\n",
        "        recent_rlist = deque(maxlen=100)\n",
        "        recent_rlist.append(0)\n",
        "\n",
        "        # 최근 100개의 점수가 195점 넘을 때까지 학습\n",
        "        while np.mean(recent_rlist) <= 195:\n",
        "            episode += 1\n",
        "            episode_memory = deque()\n",
        "            rall = 0\n",
        "            s = env.reset()\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                # 액션 선택\n",
        "                action = A2Cagent.get_action(s)\n",
        "\n",
        "                # action을 one_hot으로 표현\n",
        "                y = np.zeros(OUTPUT)\n",
        "                y[action] = 1\n",
        "\n",
        "                s1, reward, done, _ = env.step(action)\n",
        "                rall += reward\n",
        "\n",
        "                # 에피소드 메모리에 저장\n",
        "                episode_memory.append([s, y, reward])\n",
        "                s = s1\n",
        "\n",
        "                # 에피소드가 끝났을때 학습\n",
        "                if done:\n",
        "                    episode_memory = np.array(episode_memory)\n",
        "\n",
        "                    discounted_rewards = discount_rewards(np.vstack(episode_memory[:, 2]))\n",
        "\n",
        "                    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std())\n",
        "\n",
        "                    train_episodic(A2Cagent, np.vstack(episode_memory[:, 0]), np.vstack(episode_memory[:, 1]),\n",
        "                                       discounted_rewards)\n",
        "\n",
        "                    recent_rlist.append(rall)\n",
        "\n",
        "            print(\"[Episode {0:6d}] Reward: {1:4f} Recent Reward: {2:4f}\".format(episode, rall, np.mean(recent_rlist)))\n",
        "\n",
        "        play_cartpole(A2Cagent)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "[Episode      1] Reward: 22.000000 Recent Reward: 11.000000\n",
            "[Episode      2] Reward: 33.000000 Recent Reward: 18.333333\n",
            "[Episode      3] Reward: 15.000000 Recent Reward: 17.500000\n",
            "[Episode      4] Reward: 18.000000 Recent Reward: 17.600000\n",
            "[Episode      5] Reward: 46.000000 Recent Reward: 22.333333\n",
            "[Episode      6] Reward: 14.000000 Recent Reward: 21.142857\n",
            "[Episode      7] Reward: 26.000000 Recent Reward: 21.750000\n",
            "[Episode      8] Reward: 72.000000 Recent Reward: 27.333333\n",
            "[Episode      9] Reward: 16.000000 Recent Reward: 26.200000\n",
            "[Episode     10] Reward: 17.000000 Recent Reward: 25.363636\n",
            "[Episode     11] Reward: 32.000000 Recent Reward: 25.916667\n",
            "[Episode     12] Reward: 66.000000 Recent Reward: 29.000000\n",
            "[Episode     13] Reward: 15.000000 Recent Reward: 28.000000\n",
            "[Episode     14] Reward: 41.000000 Recent Reward: 28.866667\n",
            "[Episode     15] Reward: 35.000000 Recent Reward: 29.250000\n",
            "[Episode     16] Reward: 32.000000 Recent Reward: 29.411765\n",
            "[Episode     17] Reward: 20.000000 Recent Reward: 28.888889\n",
            "[Episode     18] Reward: 56.000000 Recent Reward: 30.315789\n",
            "[Episode     19] Reward: 23.000000 Recent Reward: 29.950000\n",
            "[Episode     20] Reward: 47.000000 Recent Reward: 30.761905\n",
            "[Episode     21] Reward: 48.000000 Recent Reward: 31.545455\n",
            "[Episode     22] Reward: 48.000000 Recent Reward: 32.260870\n",
            "[Episode     23] Reward: 63.000000 Recent Reward: 33.541667\n",
            "[Episode     24] Reward: 31.000000 Recent Reward: 33.440000\n",
            "[Episode     25] Reward: 67.000000 Recent Reward: 34.730769\n",
            "[Episode     26] Reward: 16.000000 Recent Reward: 34.037037\n",
            "[Episode     27] Reward: 123.000000 Recent Reward: 37.214286\n",
            "[Episode     28] Reward: 62.000000 Recent Reward: 38.068966\n",
            "[Episode     29] Reward: 40.000000 Recent Reward: 38.133333\n",
            "[Episode     30] Reward: 41.000000 Recent Reward: 38.225806\n",
            "[Episode     31] Reward: 91.000000 Recent Reward: 39.875000\n",
            "[Episode     32] Reward: 74.000000 Recent Reward: 40.909091\n",
            "[Episode     33] Reward: 164.000000 Recent Reward: 44.529412\n",
            "[Episode     34] Reward: 60.000000 Recent Reward: 44.971429\n",
            "[Episode     35] Reward: 91.000000 Recent Reward: 46.250000\n",
            "[Episode     36] Reward: 66.000000 Recent Reward: 46.783784\n",
            "[Episode     37] Reward: 64.000000 Recent Reward: 47.236842\n",
            "[Episode     38] Reward: 105.000000 Recent Reward: 48.717949\n",
            "[Episode     39] Reward: 103.000000 Recent Reward: 50.075000\n",
            "[Episode     40] Reward: 45.000000 Recent Reward: 49.951220\n",
            "[Episode     41] Reward: 81.000000 Recent Reward: 50.690476\n",
            "[Episode     42] Reward: 29.000000 Recent Reward: 50.186047\n",
            "[Episode     43] Reward: 137.000000 Recent Reward: 52.159091\n",
            "[Episode     44] Reward: 34.000000 Recent Reward: 51.755556\n",
            "[Episode     45] Reward: 53.000000 Recent Reward: 51.782609\n",
            "[Episode     46] Reward: 30.000000 Recent Reward: 51.319149\n",
            "[Episode     47] Reward: 156.000000 Recent Reward: 53.500000\n",
            "[Episode     48] Reward: 105.000000 Recent Reward: 54.551020\n",
            "[Episode     49] Reward: 173.000000 Recent Reward: 56.920000\n",
            "[Episode     50] Reward: 148.000000 Recent Reward: 58.705882\n",
            "[Episode     51] Reward: 200.000000 Recent Reward: 61.423077\n",
            "[Episode     52] Reward: 154.000000 Recent Reward: 63.169811\n",
            "[Episode     53] Reward: 130.000000 Recent Reward: 64.407407\n",
            "[Episode     54] Reward: 106.000000 Recent Reward: 65.163636\n",
            "[Episode     55] Reward: 89.000000 Recent Reward: 65.589286\n",
            "[Episode     56] Reward: 100.000000 Recent Reward: 66.192982\n",
            "[Episode     57] Reward: 110.000000 Recent Reward: 66.948276\n",
            "[Episode     58] Reward: 160.000000 Recent Reward: 68.525424\n",
            "[Episode     59] Reward: 200.000000 Recent Reward: 70.716667\n",
            "[Episode     60] Reward: 163.000000 Recent Reward: 72.229508\n",
            "[Episode     61] Reward: 77.000000 Recent Reward: 72.306452\n",
            "[Episode     62] Reward: 125.000000 Recent Reward: 73.142857\n",
            "[Episode     63] Reward: 183.000000 Recent Reward: 74.859375\n",
            "[Episode     64] Reward: 96.000000 Recent Reward: 75.184615\n",
            "[Episode     65] Reward: 109.000000 Recent Reward: 75.696970\n",
            "[Episode     66] Reward: 200.000000 Recent Reward: 77.552239\n",
            "[Episode     67] Reward: 173.000000 Recent Reward: 78.955882\n",
            "[Episode     68] Reward: 160.000000 Recent Reward: 80.130435\n",
            "[Episode     69] Reward: 200.000000 Recent Reward: 81.842857\n",
            "[Episode     70] Reward: 200.000000 Recent Reward: 83.507042\n",
            "[Episode     71] Reward: 197.000000 Recent Reward: 85.083333\n",
            "[Episode     72] Reward: 173.000000 Recent Reward: 86.287671\n",
            "[Episode     73] Reward: 161.000000 Recent Reward: 87.297297\n",
            "[Episode     74] Reward: 24.000000 Recent Reward: 86.453333\n",
            "[Episode     75] Reward: 200.000000 Recent Reward: 87.947368\n",
            "[Episode     76] Reward: 200.000000 Recent Reward: 89.402597\n",
            "[Episode     77] Reward: 136.000000 Recent Reward: 90.000000\n",
            "[Episode     78] Reward: 200.000000 Recent Reward: 91.392405\n",
            "[Episode     79] Reward: 81.000000 Recent Reward: 91.262500\n",
            "[Episode     80] Reward: 185.000000 Recent Reward: 92.419753\n",
            "[Episode     81] Reward: 66.000000 Recent Reward: 92.097561\n",
            "[Episode     82] Reward: 200.000000 Recent Reward: 93.397590\n",
            "[Episode     83] Reward: 132.000000 Recent Reward: 93.857143\n",
            "[Episode     84] Reward: 123.000000 Recent Reward: 94.200000\n",
            "[Episode     85] Reward: 165.000000 Recent Reward: 95.023256\n",
            "[Episode     86] Reward: 200.000000 Recent Reward: 96.229885\n",
            "[Episode     87] Reward: 200.000000 Recent Reward: 97.409091\n",
            "[Episode     88] Reward: 128.000000 Recent Reward: 97.752809\n",
            "[Episode     89] Reward: 164.000000 Recent Reward: 98.488889\n",
            "[Episode     90] Reward: 200.000000 Recent Reward: 99.604396\n",
            "[Episode     91] Reward: 134.000000 Recent Reward: 99.978261\n",
            "[Episode     92] Reward: 75.000000 Recent Reward: 99.709677\n",
            "[Episode     93] Reward: 89.000000 Recent Reward: 99.595745\n",
            "[Episode     94] Reward: 147.000000 Recent Reward: 100.094737\n",
            "[Episode     95] Reward: 200.000000 Recent Reward: 101.135417\n",
            "[Episode     96] Reward: 200.000000 Recent Reward: 102.154639\n",
            "[Episode     97] Reward: 200.000000 Recent Reward: 103.153061\n",
            "[Episode     98] Reward: 200.000000 Recent Reward: 104.131313\n",
            "[Episode     99] Reward: 200.000000 Recent Reward: 105.090000\n",
            "[Episode    100] Reward: 200.000000 Recent Reward: 107.090000\n",
            "[Episode    101] Reward: 200.000000 Recent Reward: 108.870000\n",
            "[Episode    102] Reward: 200.000000 Recent Reward: 110.540000\n",
            "[Episode    103] Reward: 200.000000 Recent Reward: 112.390000\n",
            "[Episode    104] Reward: 200.000000 Recent Reward: 114.210000\n",
            "[Episode    105] Reward: 200.000000 Recent Reward: 115.750000\n",
            "[Episode    106] Reward: 25.000000 Recent Reward: 115.860000\n",
            "[Episode    107] Reward: 200.000000 Recent Reward: 117.600000\n",
            "[Episode    108] Reward: 200.000000 Recent Reward: 118.880000\n",
            "[Episode    109] Reward: 44.000000 Recent Reward: 119.160000\n",
            "[Episode    110] Reward: 200.000000 Recent Reward: 120.990000\n",
            "[Episode    111] Reward: 200.000000 Recent Reward: 122.670000\n",
            "[Episode    112] Reward: 200.000000 Recent Reward: 124.010000\n",
            "[Episode    113] Reward: 200.000000 Recent Reward: 125.860000\n",
            "[Episode    114] Reward: 200.000000 Recent Reward: 127.450000\n",
            "[Episode    115] Reward: 200.000000 Recent Reward: 129.100000\n",
            "[Episode    116] Reward: 200.000000 Recent Reward: 130.780000\n",
            "[Episode    117] Reward: 200.000000 Recent Reward: 132.580000\n",
            "[Episode    118] Reward: 200.000000 Recent Reward: 134.020000\n",
            "[Episode    119] Reward: 200.000000 Recent Reward: 135.790000\n",
            "[Episode    120] Reward: 200.000000 Recent Reward: 137.320000\n",
            "[Episode    121] Reward: 200.000000 Recent Reward: 138.840000\n",
            "[Episode    122] Reward: 200.000000 Recent Reward: 140.360000\n",
            "[Episode    123] Reward: 200.000000 Recent Reward: 141.730000\n",
            "[Episode    124] Reward: 200.000000 Recent Reward: 143.420000\n",
            "[Episode    125] Reward: 200.000000 Recent Reward: 144.750000\n",
            "[Episode    126] Reward: 200.000000 Recent Reward: 146.590000\n",
            "[Episode    127] Reward: 22.000000 Recent Reward: 145.580000\n",
            "[Episode    128] Reward: 63.000000 Recent Reward: 145.590000\n",
            "[Episode    129] Reward: 200.000000 Recent Reward: 147.190000\n",
            "[Episode    130] Reward: 186.000000 Recent Reward: 148.640000\n",
            "[Episode    131] Reward: 115.000000 Recent Reward: 148.880000\n",
            "[Episode    132] Reward: 111.000000 Recent Reward: 149.250000\n",
            "[Episode    133] Reward: 182.000000 Recent Reward: 149.430000\n",
            "[Episode    134] Reward: 189.000000 Recent Reward: 150.720000\n",
            "[Episode    135] Reward: 181.000000 Recent Reward: 151.620000\n",
            "[Episode    136] Reward: 200.000000 Recent Reward: 152.960000\n",
            "[Episode    137] Reward: 200.000000 Recent Reward: 154.320000\n",
            "[Episode    138] Reward: 136.000000 Recent Reward: 154.630000\n",
            "[Episode    139] Reward: 31.000000 Recent Reward: 153.910000\n",
            "[Episode    140] Reward: 168.000000 Recent Reward: 155.140000\n",
            "[Episode    141] Reward: 200.000000 Recent Reward: 156.330000\n",
            "[Episode    142] Reward: 200.000000 Recent Reward: 158.040000\n",
            "[Episode    143] Reward: 196.000000 Recent Reward: 158.630000\n",
            "[Episode    144] Reward: 163.000000 Recent Reward: 159.920000\n",
            "[Episode    145] Reward: 162.000000 Recent Reward: 161.010000\n",
            "[Episode    146] Reward: 169.000000 Recent Reward: 162.400000\n",
            "[Episode    147] Reward: 176.000000 Recent Reward: 162.600000\n",
            "[Episode    148] Reward: 200.000000 Recent Reward: 163.550000\n",
            "[Episode    149] Reward: 200.000000 Recent Reward: 163.820000\n",
            "[Episode    150] Reward: 200.000000 Recent Reward: 164.340000\n",
            "[Episode    151] Reward: 179.000000 Recent Reward: 164.130000\n",
            "[Episode    152] Reward: 193.000000 Recent Reward: 164.520000\n",
            "[Episode    153] Reward: 169.000000 Recent Reward: 164.910000\n",
            "[Episode    154] Reward: 200.000000 Recent Reward: 165.850000\n",
            "[Episode    155] Reward: 142.000000 Recent Reward: 166.380000\n",
            "[Episode    156] Reward: 200.000000 Recent Reward: 167.380000\n",
            "[Episode    157] Reward: 200.000000 Recent Reward: 168.280000\n",
            "[Episode    158] Reward: 140.000000 Recent Reward: 168.080000\n",
            "[Episode    159] Reward: 200.000000 Recent Reward: 168.080000\n",
            "[Episode    160] Reward: 141.000000 Recent Reward: 167.860000\n",
            "[Episode    161] Reward: 149.000000 Recent Reward: 168.580000\n",
            "[Episode    162] Reward: 200.000000 Recent Reward: 169.330000\n",
            "[Episode    163] Reward: 187.000000 Recent Reward: 169.370000\n",
            "[Episode    164] Reward: 167.000000 Recent Reward: 170.080000\n",
            "[Episode    165] Reward: 158.000000 Recent Reward: 170.570000\n",
            "[Episode    166] Reward: 200.000000 Recent Reward: 170.570000\n",
            "[Episode    167] Reward: 200.000000 Recent Reward: 170.840000\n",
            "[Episode    168] Reward: 166.000000 Recent Reward: 170.900000\n",
            "[Episode    169] Reward: 192.000000 Recent Reward: 170.820000\n",
            "[Episode    170] Reward: 200.000000 Recent Reward: 170.820000\n",
            "[Episode    171] Reward: 200.000000 Recent Reward: 170.850000\n",
            "[Episode    172] Reward: 200.000000 Recent Reward: 171.120000\n",
            "[Episode    173] Reward: 188.000000 Recent Reward: 171.390000\n",
            "[Episode    174] Reward: 200.000000 Recent Reward: 173.150000\n",
            "[Episode    175] Reward: 199.000000 Recent Reward: 173.140000\n",
            "[Episode    176] Reward: 200.000000 Recent Reward: 173.140000\n",
            "[Episode    177] Reward: 200.000000 Recent Reward: 173.780000\n",
            "[Episode    178] Reward: 200.000000 Recent Reward: 173.780000\n",
            "[Episode    179] Reward: 147.000000 Recent Reward: 174.440000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ae27583a6449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ae27583a6449>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;31m# 액션 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA2Cagent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;31m# action을 one_hot으로 표현\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ae27583a6449>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mstate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0maction_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate_t\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# 각 액션의 확률로 액션을 결정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQPpjAixeD8zcenj9qdiwd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yougoal/leejunghoon/blob/master/dqn2015last.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk7HgTUr_rSj",
        "colab_type": "code",
        "outputId": "70444b6d-8224-4607-cc04-d4ff0c1c21cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from gym import wrappers\n",
        "import numpy as np\n",
        "import random as ran\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# 꺼내서 사용할 리플레이 갯수\n",
        "REPLAY = 3\n",
        "# 리플레이를 저장할 리스트\n",
        "REPLAY_MEMORY = []\n",
        "# 미니배치\n",
        "MINIBATCH = 6\n",
        "\n",
        "INPUT = env.observation_space.shape[0]\n",
        "OUTPUT = env.action_space.n\n",
        "\n",
        "# 하이퍼파라미터\n",
        "LEARNING_LATE = 0.001\n",
        "DISCOUNT = 1\n",
        "model_path = \"save/model.ckpt\"\n",
        "\n",
        "\n",
        "# 두개의 네트워크 구성\n",
        "\n",
        "x=tf.placeholder(dtype=tf.float32, shape=(None, 4))\n",
        "Y=tf.placeholder(dtype=tf.float32, shape=(None, 2))\n",
        "\n",
        "\n",
        "# Main 네트워크\n",
        "W1 = tf.get_variable('W1',shape=[4,2],initializer=tf.contrib.layers.xavier_initializer())\n",
        "Q_pre = tf.matmul(x,W1)\n",
        "\n",
        "# Target 네트워크\n",
        "W1_r = tf.get_variable('W1_r',shape=[4,2],initializer=tf.contrib.layers.xavier_initializer())\n",
        "Q_pre_r = tf.matmul(x,W1_r)\n",
        "\n",
        "# 총 Reward를 저장해놓을 리스트\n",
        "rlist=[0]\n",
        "recent_rlist=[0]\n",
        "\n",
        "episode = 0\n",
        "\n",
        "# Loss function 정의\n",
        "cost = tf.reduce_sum(tf.square(Y-Q_pre))\n",
        "optimizer = tf.train.AdamOptimizer(LEARNING_LATE, epsilon=0.01)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# 세션 정의\n",
        "with tf.Session() as sess:\n",
        "    # 변수 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Target 네트워크에 main 네트워크 값을 카피해줌\n",
        "    sess.run(W1_r.assign(W1))\n",
        "    print(\"초기시작시 Wm과 Wt확인\")\n",
        "    print(sess.run(W1_r.assign(W1)))\n",
        "    print(sess.run(W1))\n",
        "\n",
        "    # 에피소드 시작\n",
        "    while np.mean(recent_rlist) <10 :\n",
        "        episode += 1\n",
        "        print(\"Episdode마다 Wm과 Wt확인\")\n",
        "        print('Episode :{}'.format(episode))\n",
        "        print(sess.run(W1_r.assign(W1)))\n",
        "        print(sess.run(W1))\n",
        "        # state 초기화\n",
        "        s = env.reset()\n",
        "        if len(recent_rlist) > 5:\n",
        "            del recent_rlist[0]\n",
        "        print(\"recent_rlist\")\n",
        "        print(recent_rlist)\n",
        "        # e-greedy\n",
        "        e = 1. / ((episode/25)+1)\n",
        "\n",
        "        rall = 0\n",
        "        d = False\n",
        "        count = 0\n",
        "\n",
        "        # 에피소드가 끝나기 전까지 반복\n",
        "        while not d  :\n",
        "\n",
        "            #env.render()\n",
        "            count += 1\n",
        "\n",
        "            # state 값의 전처리\n",
        "            s_t = np.reshape(s,[1,INPUT])\n",
        "            # 현재 상태의 Q값을 에측\n",
        "            Q = sess.run(Q_pre, feed_dict={x:s_t})\n",
        "            # e-greedy 정책으로 랜덤하게 action 결정\n",
        "            if e > np.random.rand(1):\n",
        "                a = env.action_space.sample()\n",
        "            else:\n",
        "                a = np.argmax(Q)\n",
        "\n",
        "            # 결정된 action으로 Environment에 입력\n",
        "            s1, r, d, _ = env.step(a)\n",
        "\n",
        "            # Environment에서 반환한 Next_state, action, reward, done 값들을\n",
        "            # Replay_memory에 저장\n",
        "            REPLAY_MEMORY.append([s_t,a,r,s1,d,count])\n",
        "\n",
        "            # 저장된 값들이 50000개 이상 넘어가면 맨 앞 Replay부터 삭제\n",
        "            if len(REPLAY_MEMORY) > 6:\n",
        "                del REPLAY_MEMORY[0]\n",
        "            print('메모리 버퍼에 저장된 값 출력')\n",
        "           # print(REPLAY_MEMORY[0:1])\n",
        "            #print(REPLAY_MEMORY[1:2])\n",
        "            #print(REPLAY_MEMORY[2:3])\n",
        "            #print(REPLAY_MEMORY[3:4])\n",
        "            #print(REPLAY_MEMORY[4:5])\n",
        "            #print(REPLAY_MEMORY[5:6])\n",
        "            # 총 reward 합\n",
        "            rall += r\n",
        "            # state를 Next_state로 바꿈\n",
        "            s = s1\n",
        "\n",
        "\n",
        "        # 일정간격의 episode마다 학습을 수행한다.\n",
        "        if episode % 5 == 2 :\n",
        "\n",
        "            # 50번의 미니배치로 학습\n",
        "                # 저장된 리플레이 중에 학습에 사용할 랜덤한 리플레이 샘플들을 가져옴\n",
        "            for sample in ran.sample(REPLAY_MEMORY, 3):\n",
        "                s_t_r, a_r, r_r, s1_r, d_r ,count_r= sample\n",
        "                print(\"메모리버퍼에서 꺼내온 샘플 Target Str At Rt St+1 done count_R\")\n",
        "                print(sample)\n",
        "                # 꺼내온 리플레이의 state의 Q값을 예측\n",
        "                Q = sess.run(Q_pre, feed_dict={x: s_t_r})\n",
        "                print(\"메모리버퍼에서 꺼내온 St로 Q(St,a)를 예측 Qpre=st*Wm\")\n",
        "                print(sess.run(W1))\n",
        "                if d_r:\n",
        "                    # 꺼내온 리플레이의 상태가 끝난 상황이라면 Negative Reward를 부여\n",
        "                        Q[0, a_r] = -100\n",
        "                else:\n",
        "                    # 끝나지 않았다면 Q값을 업데이트\n",
        "                    # 꺼내온 st+1로 표적의 값을 만들어낸다\n",
        "                    s1_t_r= np.reshape(s1_r,[1,4])\n",
        "                    Q1 = sess.run(Q_pre_r, feed_dict={x: s1_t_r})\n",
        "                    print(\"y(st.a)에 사용된 Wt\")\n",
        "                    print(sess.run(W1))\n",
        "                    Q[0, a_r]=np.max(Q1)\n",
        "                    # 표적값\n",
        "                    print(\"표적 Q(st+1,at)==>y(st,a)=st+1*Wt, Q1 및 Q\")\n",
        "                    print(Q1,Q)\n",
        "                    \n",
        "                # 업데이트 된 Q값으로 main네트워크를 학습\n",
        "                _, loss = sess.run([train, cost], feed_dict={x: s_t_r, Y: Q })\n",
        "                print(\"훈련에참여한 값 St{}\".format(s_t_r))\n",
        "                print(\"훈련에참여한 값 Target{}\".format(Q))\n",
        "                print(\" REPLY 훈련W값\")\n",
        "                print(sess.run([W1]))\n",
        "                print(\"최종 loss{}\".format(loss))\n",
        "                \n",
        "            # 10번 마다 target 네트워크에 main 네트워크 값을 copy\n",
        "            sess.run(W1_r.assign(W1))\n",
        "            print(\"Wm==>Wt로 복사\")\n",
        "            print(sess.run(W1_r.assign(W1)))\n",
        "        # 총 reward의 합을 list에 저장\n",
        "        recent_rlist.append(rall)\n",
        "        rlist.append(rall)\n",
        "      #  print(\"Episode:{} steps:{} reward:{} average reward:{} recent reward:{}\".format(episode, count, rall,\n",
        "        #                                                                                np.mean(rlist),\n",
        "         #                                                                               np.mean(recent_rlist)))\n",
        "\n",
        "    save_path = saver.save(sess, model_path)\n",
        "    print(\"Model saved in file: \",save_path)\n",
        "\n",
        "\n",
        "    rlist=[]\n",
        "    recent_rlist=[]\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver.restore(sess, model_path)\n",
        "\n",
        "    print(\"Model restored form file: \", save_path)\n",
        "    for episode in range(10):\n",
        "        # state 초기화\n",
        "        s = env.reset()\n",
        "\n",
        "        rall = 0\n",
        "        d = False\n",
        "        count = 0\n",
        "        # 에피소드가 끝나기 전까지 반복\n",
        "        while not d :\n",
        "           # env.render()\n",
        "            count += 1\n",
        "            # state 값의 전처리\n",
        "            s_t = np.reshape(s, [1, INPUT])\n",
        "\n",
        "            # 현재 상태의 Q값을 에측\n",
        "            Q = sess.run(Q_pre, feed_dict={x: s_t})\n",
        "            a = np.argmax(Q)\n",
        "\n",
        "            # 결정된 action으로 Environment에 입력\n",
        "            s, r, d, _ = env.step(a)\n",
        "\n",
        "            # 총 reward 합\n",
        "            rall += r\n",
        "\n",
        "\n",
        "        rlist.append(rall)\n",
        "\n",
        "       # print(\"Episode : {} steps : {} r={}. averge reward : {}\".format(episode, count, rall,\n",
        "       #                                                                 np.mean(rlist)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "초기시작시 Wm과 Wt확인\n",
            "[[ 0.7929151  -0.2949438 ]\n",
            " [ 0.6067889  -0.3838911 ]\n",
            " [ 0.79167914  0.49992323]\n",
            " [-0.29413843  0.5096917 ]]\n",
            "[[ 0.7929151  -0.2949438 ]\n",
            " [ 0.6067889  -0.3838911 ]\n",
            " [ 0.79167914  0.49992323]\n",
            " [-0.29413843  0.5096917 ]]\n",
            "Episdode마다 Wm과 Wt확인\n",
            "Episode :1\n",
            "[[ 0.7929151  -0.2949438 ]\n",
            " [ 0.6067889  -0.3838911 ]\n",
            " [ 0.79167914  0.49992323]\n",
            " [-0.29413843  0.5096917 ]]\n",
            "[[ 0.7929151  -0.2949438 ]\n",
            " [ 0.6067889  -0.3838911 ]\n",
            " [ 0.79167914  0.49992323]\n",
            " [-0.29413843  0.5096917 ]]\n",
            "recent_rlist\n",
            "[0]\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "Episdode마다 Wm과 Wt확인\n",
            "Episode :2\n",
            "[[ 0.7929151  -0.2949438 ]\n",
            " [ 0.6067889  -0.3838911 ]\n",
            " [ 0.79167914  0.49992323]\n",
            " [-0.29413843  0.5096917 ]]\n",
            "[[ 0.7929151  -0.2949438 ]\n",
            " [ 0.6067889  -0.3838911 ]\n",
            " [ 0.79167914  0.49992323]\n",
            " [-0.29413843  0.5096917 ]]\n",
            "recent_rlist\n",
            "[0, 18.0]\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리버퍼에서 꺼내온 샘플 Target Str At Rt St+1 done count_R\n",
            "[array([[-0.04627627, -0.97781635,  0.12259422,  1.5696823 ]]), 1, 1.0, array([-0.06583259, -0.78435275,  0.15398787,  1.31761738]), False, 8]\n",
            "메모리버퍼에서 꺼내온 St로 Q(St,a)를 예측 Qpre=st*Wm\n",
            "[[ 0.7929151  -0.2949438 ]\n",
            " [ 0.6067889  -0.3838911 ]\n",
            " [ 0.79167914  0.49992323]\n",
            " [-0.29413843  0.5096917 ]]\n",
            "y(st.a)에 사용된 Wt\n",
            "[[ 0.7929151  -0.2949438 ]\n",
            " [ 0.6067889  -0.3838911 ]\n",
            " [ 0.79167914  0.49992323]\n",
            " [-0.29413843  0.5096917 ]]\n",
            "표적 Q(st+1,at)==>y(st,a)=st+1*Wt, Q1 및 Q\n",
            "[[-0.79378915  1.0690838 ]] [[-0.9946698  1.0690838]]\n",
            "훈련에참여한 값 St[[-0.04627627 -0.97781635  0.12259422  1.5696823 ]]\n",
            "훈련에참여한 값 Target[[-0.9946698  1.0690838]]\n",
            " REPLY 훈련W값\n",
            "[array([[ 0.7929151 , -0.2948934 ],\n",
            "       [ 0.6067889 , -0.38336256],\n",
            "       [ 0.79167914,  0.4998    ],\n",
            "       [-0.29413843,  0.5090489 ]], dtype=float32)]\n",
            "최종 loss0.03286309167742729\n",
            "메모리버퍼에서 꺼내온 샘플 Target Str At Rt St+1 done count_R\n",
            "[array([[-0.08151965, -0.98104999,  0.18034021,  1.65426576]]), 1, 1.0, array([-0.10114065, -0.78843322,  0.21342553,  1.42275928]), True, 10]\n",
            "메모리버퍼에서 꺼내온 St로 Q(St,a)를 예측 Qpre=st*Wm\n",
            "[[ 0.7929151  -0.2948934 ]\n",
            " [ 0.6067889  -0.38336256]\n",
            " [ 0.79167914  0.4998    ]\n",
            " [-0.29413843  0.5090489 ]]\n",
            "훈련에참여한 값 St[[-0.08151965 -0.98104999  0.18034021  1.65426576]]\n",
            "훈련에참여한 값 Target[[  -1.00374 -100.     ]]\n",
            " REPLY 훈련W값\n",
            "[array([[ 0.7929151 , -0.2941626 ],\n",
            "       [ 0.6067889 , -0.38261843],\n",
            "       [ 0.79167914,  0.49906144],\n",
            "       [-0.29413843,  0.5083043 ]], dtype=float32)]\n",
            "최종 loss10268.25\n",
            "메모리버퍼에서 꺼내온 샘플 Target Str At Rt St+1 done count_R\n",
            "[array([[-0.03064451, -0.78158766,  0.09763257,  1.2480826 ]]), 0, 1.0, array([-0.04627627, -0.97781635,  0.12259422,  1.5696823 ]), False, 7]\n",
            "메모리버퍼에서 꺼내온 St로 Q(St,a)를 예측 Qpre=st*Wm\n",
            "[[ 0.7929151  -0.2941626 ]\n",
            " [ 0.6067889  -0.38261843]\n",
            " [ 0.79167914  0.49906144]\n",
            " [-0.29413843  0.5083043 ]]\n",
            "y(st.a)에 사용된 Wt\n",
            "[[ 0.7929151  -0.2941626 ]\n",
            " [ 0.6067889  -0.38261843]\n",
            " [ 0.79167914  0.49906144]\n",
            " [-0.29413843  0.5083043 ]]\n",
            "표적 Q(st+1,at)==>y(st,a)=st+1*Wt, Q1 및 Q\n",
            "[[-0.9946698  1.2503656]] [[1.2503656 0.9911947]]\n",
            "훈련에참여한 값 St[[-0.03064451 -0.78158766  0.09763257  1.2480826 ]]\n",
            "훈련에참여한 값 Target[[1.2503656 0.9911947]]\n",
            " REPLY 훈련W값\n",
            "[array([[ 0.7927342 , -0.29359767],\n",
            "       [ 0.6062077 , -0.3820432 ],\n",
            "       [ 0.79203516,  0.49849054],\n",
            "       [-0.293537  ,  0.50772876]], dtype=float32)]\n",
            "최종 loss4.156453609466553\n",
            "Wm==>Wt로 복사\n",
            "[[ 0.7927342  -0.29359767]\n",
            " [ 0.6062077  -0.3820432 ]\n",
            " [ 0.79203516  0.49849054]\n",
            " [-0.293537    0.50772876]]\n",
            "Episdode마다 Wm과 Wt확인\n",
            "Episode :3\n",
            "[[ 0.7927342  -0.29359767]\n",
            " [ 0.6062077  -0.3820432 ]\n",
            " [ 0.79203516  0.49849054]\n",
            " [-0.293537    0.50772876]]\n",
            "[[ 0.7927342  -0.29359767]\n",
            " [ 0.6062077  -0.3820432 ]\n",
            " [ 0.79203516  0.49849054]\n",
            " [-0.293537    0.50772876]]\n",
            "recent_rlist\n",
            "[0, 18.0, 10.0]\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "메모리 버퍼에 저장된 값 출력\n",
            "Model saved in file:  save/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from save/model.ckpt\n",
            "Model restored form file:  save/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
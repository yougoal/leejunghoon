{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMLP68e605IdeHsAP7bovrL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yougoal/leejunghoon/blob/master/pGtest1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFUkibfH79Oe",
        "colab_type": "code",
        "outputId": "8bd5b0e7-9cf8-4750-ff96-6dbc4fbe5648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from collections import deque\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# 하이퍼 파라미터\n",
        "LEARNING_RATE = 0.005\n",
        "INPUT = env.observation_space.shape[0]\n",
        "OUTPUT = env.action_space.n\n",
        "DISCOUNT = 0.99\n",
        "\n",
        "\n",
        "def discount_rewards(r):\n",
        "    '''Discounted reward를 구하기 위한 함수\n",
        "    \n",
        "    Args:\n",
        "         r(np.array): reward 값이 저장된 array\n",
        "    \n",
        "    Returns:\n",
        "        discounted_r(np.array): Discounted 된 reward가 저장된 array\n",
        "    '''\n",
        "    discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "    print(\"여기를 \")\n",
        "    #print(discounted_r)\n",
        "    running_add = 0\n",
        "    for t in reversed(range(len(r))):\n",
        "        running_add = running_add * DISCOUNT + r[t]\n",
        "        discounted_r[t] = running_add\n",
        "\n",
        "    return discounted_r\n",
        "\n",
        "\n",
        "def train_episodic(PGagent, x, y, adv):\n",
        "    '''에피소드당 학습을 하기위한 함수\n",
        "    \n",
        "    Args:\n",
        "        PGagent(PolicyGradient): 학습될 네트워크\n",
        "        x(np.array): State가 저장되어있는 array\n",
        "        y(np.array): Action(one_hot)이 저장되어있는 array\n",
        "        adv(np.array) : Discounted reward가 저장되어있는 array\n",
        "        \n",
        "    Returns:\n",
        "        l(float): 네트워크에 의한 loss\n",
        "    '''\n",
        "    l,_ = PGagent.sess.run([PGagent.loss, PGagent.train], feed_dict={PGagent.X: x, PGagent.Y: y, PGagent.adv : adv})\n",
        "    return l\n",
        "\n",
        "def play_cartpole(PGagent):\n",
        "    '''학습된 네트워크로 Play하기 위한 함수\n",
        "    \n",
        "    Args:\n",
        "         PGagent(PolicyGradient): 학습된 네트워크\n",
        "    '''\n",
        "    print(\"Play Cartpole!\")\n",
        "    episode = 0\n",
        "    while True:\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        rall = 0\n",
        "        episode += 1\n",
        "        while not done:\n",
        "            env.render()\n",
        "            action_p = PGagent.sess.run(PGagent.a_pre, feed_dict={PGagent.X : s})\n",
        "            s1, reward, done, _ = env.step(np.argmax(action_p))\n",
        "            s = s1\n",
        "            rall += reward\n",
        "        print(\"[Episode {0:6f}] Reward: {1:4f} \".format(episode, rall))\n",
        "\n",
        "class PolicyGradient:\n",
        "    def __init__(self, sess, input_size, output_size):\n",
        "        self.sess = sess\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.build_network()\n",
        "\n",
        "    def build_network(self):\n",
        "        self.X = tf.placeholder('float',[None, self.input_size])\n",
        "        self.Y = tf.placeholder('float', [None, self.output_size])\n",
        "        self.adv = tf.placeholder('float')\n",
        "\n",
        "        W1 = tf.get_variable('W1',shape=[4,2],initializer=tf.contrib.layers.xavier_initializer())\n",
        "        \n",
        "        self.a_pre = tf.nn.softmax(tf.matmul(self.X,W1))\n",
        "        self.log_p = self.Y * tf.log(self.a_pre)\n",
        "        self.log_lik = self.log_p * self.adv\n",
        "        #print(\"중간과정프린트\")\n",
        "        #print(self.log_p)\n",
        "        # loss=reduce_sum(-Action*log(softmax(st*W))*Q) 이것을 계산함.\n",
        "        self.loss = tf.reduce_mean(tf.reduce_sum(-self.log_lik, axis=1))\n",
        "        self.train = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.loss)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state_t = np.reshape(state, [1, self.input_size])\n",
        "        action_p = self.sess.run(self.a_pre, feed_dict={self.X : state_t})\n",
        "\n",
        "        # 각 액션의 확률로 액션을 결정\n",
        "        action = np.random.choice(np.arange(self.output_size), p=action_p[0])\n",
        "\n",
        "        return action\n",
        "\n",
        "def main():\n",
        "    with tf.Session() as sess:\n",
        "        PGagent = PolicyGradient(sess, INPUT, OUTPUT)\n",
        "        \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        episode = 0\n",
        "        recent_rlist = deque(maxlen=100)\n",
        "        recent_rlist.append(0)\n",
        "\n",
        "        # 최근 100개의 점수가 195점 넘을 때까지 학습\n",
        "        while np.mean(recent_rlist) <= 30:\n",
        "            episode += 1\n",
        "            episode_memory = deque()\n",
        "            rall = 0\n",
        "            s = env.reset()\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                # 액션 선택\n",
        "                action = PGagent.get_action(s)\n",
        "                # action을 one_hot으로 표현\n",
        "                y = np.zeros(OUTPUT)\n",
        "                y[action] = 1\n",
        "                s1, reward, done, _ = env.step(action)\n",
        "                rall += reward\n",
        "\n",
        "                # 에피소드 메모리에 저장\n",
        "                episode_memory.append([s, y, reward])\n",
        "             \n",
        "                s = s1\n",
        "\n",
        "                # 에피소드가 끝났을때 학습\n",
        "                if done:\n",
        "                    print(\"Episode\")\n",
        "                    #print(episode)\n",
        "                    print(\"ARRARY 이전 에피소드메모리\")\n",
        "                    #print(episode_memory)\n",
        "                    episode_memory = np.array(episode_memory)\n",
        "                    #print(\"에피소드메모리\")\n",
        "                    #print(episode_memory)\n",
        "                    #print(\"에피소드메모리[:,2]\")\n",
        "                    #print(episode_memory[:,2])\n",
        "                    discounted_rewards = discount_rewards(np.vstack(episode_memory[:,2]))\n",
        "                   # print(\"vv에피소드메모리\")\n",
        "                   # print(np.vstack(episode_memory[:,2]))\n",
        "                    # 리워드가 1이므로 양의 값을 증가하는 방향을 가지므로 디스카운터 리워드를 정규화시켜 출력함. 값=(값-평균)/표준편차\n",
        "                    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)\n",
        "                    \n",
        "                    print(\"저장된 것들의 디스카운터리워드\")\n",
        "                    #print(discounted_rewards)   첫번째는 상태 s                   두번째는 상태는 action을 one hot으로 1인경우 저장 [0 , 1]과 같이\n",
        "                    #                                                                   세번재는 메로리의 합의 디스카운트 리워드\n",
        "                    l = train_episodic(PGagent, np.vstack(episode_memory[:,0]), np.vstack(episode_memory[:,1]),discounted_rewards)\n",
        "                    print(\"중간과정프린트\")\n",
        "                    print(sess.run(self.log_p))\n",
        "          \n",
        "            print(\"[Episode {0:6f}] Reward: {1:4f} Loss: {2:5.5f} Recent Reward: {3:4f}\".format(episode, rall, l,\n",
        "                                                                                                np.mean(recent_rlist)))\n",
        "\n",
        "        play_cartpole(PGagent)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode\n",
            "ARRARY 이전 에피소드메모리\n",
            "여기를 \n",
            "저장된 것들의 디스카운터리워드\n",
            "중간과정프린트\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d69d138ee3e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-d69d138ee3e2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_episodic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPGagent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_memory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_memory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscounted_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"중간과정프린트\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             print(\"[Episode {0:6f}] Reward: {1:4f} Loss: {2:5.5f} Recent Reward: {3:4f}\".format(episode, rall, l,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    }
  ]
}
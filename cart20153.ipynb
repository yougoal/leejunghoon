{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP5oU0+U6CiMfl+YLj5gI6x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yougoal/leejunghoon/blob/master/cart20153.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnS-gBNB2XBS",
        "colab_type": "code",
        "outputId": "b266e9e2-fb09-49ba-f629-11ed64fe0878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "#참조: https://hunkim.github.io/ml/\n",
        "#setup\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import dqn\n",
        "from collections import deque\n",
        "\n",
        "import gym\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "\n",
        "dis = 0.9\n",
        "REPLAY_MEMORY = 50000\n",
        "\n",
        "\n",
        "def replay_train(mainDQN, targetDQN, train_batch):\n",
        "    x_stack = np.empty(0).reshape(0, input_size)\n",
        "    y_stack = np.empty(0).reshape(0, output_size)\n",
        "\n",
        "    for state, action, reward, next_state, done in train_batch:\n",
        "        Q = mainDQN.predict(state)\n",
        "\n",
        "        if done:\n",
        "            Q[0, action] = reward\n",
        "        else:\n",
        "            Q[0, action] = reward + dis * np.max(targetDQN.predict(next_state))\n",
        "\n",
        "        y_stack = np.vstack([y_stack, Q])\n",
        "        x_stack = np.vstack([x_stack, state])\n",
        "\n",
        "    return mainDQN.update(x_stack, y_stack)\n",
        "\n",
        "def bot_play(mainDQN):\n",
        "\n",
        "    s = env.reset()\n",
        "    reward_sum = 0\n",
        "\n",
        "    while True:\n",
        "        env.render()\n",
        "        a = np.argmax(mainDQN.predict(s))\n",
        "        s, reward, done, _ = env.step(a)\n",
        "        reward_sum += reward\n",
        "        if done:\n",
        "            print(\"Total score: {}\".format(reward_sum))\n",
        "            break\n",
        "\n",
        "def get_copy_var_ops(*, dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
        "    op_holder = []\n",
        "    src_vals = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = src_scope_name)\n",
        "    dest_vals = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = dest_scope_name)\n",
        "\n",
        "    for src_vals, dest_vals in zip(src_vals, dest_vals):\n",
        "        op_holder.append(dest_vals.assign(src_vals.value()))\n",
        "    return op_holder\n",
        "\n",
        "def main():\n",
        "    max_episodes = 1000\n",
        "\n",
        "    replay_buffer = deque()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        mainDQN = dqn.DQN(sess, input_size, output_size, name=\"main\")#dqn.DQN(sess, input_size, output_size)\n",
        "        targetDQN = dqn.DQN(sess, input_size, output_size, name=\"target\")\n",
        "        tf.global_variables_initializer().run()\n",
        "        copy_ops = get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\")\n",
        "        sess.run(copy_ops)\n",
        "        for episode in range(max_episodes):\n",
        "            e = 1. / ((episode / 10) + 1)\n",
        "            done = False\n",
        "            step_count = 0\n",
        "\n",
        "            state = env.reset()\n",
        "\n",
        "            while not done:\n",
        "\n",
        "                if np.random.rand(1) < e :\n",
        "                    action = env.action_space.sample()\n",
        "                else:\n",
        "                    action = np.argmax(mainDQN.predict(state))\n",
        "\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                if done:\n",
        "                    reward = -100\n",
        "\n",
        "                replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "                if len(replay_buffer) > REPLAY_MEMORY:\n",
        "                    replay_buffer.popleft()\n",
        "\n",
        "                state = next_state\n",
        "                step_count += 1\n",
        "                if step_count > 10000:\n",
        "                    break\n",
        "\n",
        "            print(\"episode: {}  step: {}\".format(episode, step_count))\n",
        "\n",
        "            if step_count > 10000:\n",
        "                pass\n",
        "\n",
        "            if episode % 10 == 1:\n",
        "                for _ in range(50):\n",
        "                    minibatch = random.sample(replay_buffer, 10)\n",
        "                    loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n",
        "                print(\"Loss: \", loss)\n",
        "                sess.run(copy_ops)\n",
        "\n",
        "        bot_play(mainDQN)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f85de9a91f70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dqn'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}